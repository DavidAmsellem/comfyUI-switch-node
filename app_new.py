#!/usr/bin/env python3
"""
API REST NUEVA - Implementaci√≥n simplificada desde cero
"""
import os
import json
import uuid
import random
import copy
import time
import shutil
import threading
import concurrent.futures
import stat
from datetime import datetime
from io import BytesIO

from flask import Flask, request, jsonify, send_file
from flask_cors import CORS
from PIL import Image
import requests
from werkzeug.utils import secure_filename

# Importar configuraci√≥n de estilos
from style_presets import get_available_styles, apply_style_to_workflow, get_workflow_nodes_for_style

# Importar sistema de persistencia de sesi√≥n
from job_persistence import session_manager

# Importar sistema de cancelaci√≥n de trabajos ComfyUI
from comfyui_cancel import cancel_all_comfyui_jobs, get_comfyui_queue_status, cancel_specific_comfyui_job

# Configuraci√≥n b√°sica
app = Flask(__name__)
CORS(app)

# URLs y directorios
COMFYUI_HOST = os.getenv('COMFYUI_HOST', 'localhost')
COMFYUI_PORT = os.getenv('COMFYUI_PORT', '8188')
COMFYUI_URL = f"http://{COMFYUI_HOST}:{COMFYUI_PORT}"

# Sistema de tracking de batches en progreso
ACTIVE_BATCHES = {}  # batch_id -> batch_info
BATCH_LOCK = threading.Lock()

# Sistema de control de throttling para batches
BATCH_THROTTLE_LOCK = threading.Lock()
LAST_BATCH_SUBMIT_TIME = 0
BATCH_PROMPT_SEND_DELAY = 0  # Sin delay entre prompts del mismo lote
CURRENT_BATCH_PROMPTS = 0  # N√∫mero de prompts del lote actual

# Directorios principales (simplificados)
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
COMFYUI_ROOT = os.path.abspath(os.path.join(BASE_DIR, '..', '..'))
COMFYUI_INPUT_DIR = os.path.join(COMFYUI_ROOT, 'input')
COMFYUI_OUTPUT_DIR = os.path.join(COMFYUI_ROOT, 'output')  # ComfyUI output (solo para leer)
WORKFLOWS_DIR = os.path.join(BASE_DIR, 'workflows')
TEMP_UPLOADS_DIR = os.path.join(BASE_DIR, 'temp_uploads')

# üéØ NUESTRO DIRECTORIO DE SALIDA PERSONALIZADO (independiente de ComfyUI)
OUR_OUTPUT_DIR = os.path.join(os.path.dirname(COMFYUI_ROOT), 'output')  # C:\Users\david_qskhc9c\Documents\ComfyUI_windows_portable\output

# Crear directorios necesarios
for directory in [COMFYUI_INPUT_DIR, COMFYUI_OUTPUT_DIR, TEMP_UPLOADS_DIR, OUR_OUTPUT_DIR]:
    os.makedirs(directory, exist_ok=True)

# Configuraci√≥n del workflow (actualizar seg√∫n tu nuevo workflow)
WORKFLOW_CONFIG = {
    'load_image_node_id': '699',  # ID del nodo LoadImage para el cuadro
    'save_image_node_id': '704',  # ID del nodo SaveImage principal
    'frame_node_id': '692',       # ID del nodo DynamicFrameNode
    'allowed_extensions': ['png', 'jpg', 'jpeg', 'gif', 'bmp', 'webp'],
     'frame_colors': ['none', 'black', 'white', 'brown', 'gold'],  # Incluir 'none' para sin marco
    # Par√°metros por defecto para DynamicFrameNodeImproved (nodo mejorado con profundidad)
    'frame_node_defaults': {
        'frame_width': 50,         # Ancho del marco (0-200)
        'depth_enabled': True,     # Profundidad ACTIVADA por defecto
        'depth_intensity': 0.8,    # Intensidad de profundidad (0.1-1.0)
        'perspective_style': 'realistic',  # Estilo de perspectiva ("realistic", "subtle", "dramatic")
        'wall_color': 240,         # Color de pared (200-255) - gris claro
        'upscale_workflow': False  # Para compatibilidad con workflow de upscale
    }
}

# ==================== FUNCIONES UTILITARIAS ====================

def get_perspective_style_for_style(style_id):
    """
    Retorna el estilo de perspectiva apropiado seg√∫n el estilo seleccionado
    """
    style_perspective_styles = {
        'default': 'realistic',
        'casa_ciudad': 'realistic',
        'casa_campo': 'subtle',
        'casa_playa': 'subtle',
        'casa_montana': 'dramatic',
        'casa_moderna': 'realistic',
        'minimalist': 'subtle',
        'luxury': 'dramatic',
        'industrial': 'realistic',
        'warm_cozy': 'subtle',
        'futuristic': 'realistic',
        'artistic_bohemian': 'dramatic'
    }
    
    return style_perspective_styles.get(style_id, 'realistic')  # Default: realistic

def get_wall_color_for_style(style_id):
    """
    Retorna el color de pared apropiado seg√∫n el estilo seleccionado
    """
    style_wall_colors = {
        # Estilos normales (conservadores)
        'default': 240,        # Gris claro neutro
        'casa_ciudad': 235,    # Gris urbano
        'casa_campo': 245,     # Blanco c√°lido
        'casa_playa': 250,     # Blanco costero
        'casa_montana': 230,   # Gris piedra
        'casa_moderna': 240,   # Gris moderno
        
        # Estilos creativos (m√°s dram√°ticos)
        'minimalist': 252,     # Blanco puro
        'luxury': 235,         # Gris elegante
        'industrial': 220,     # Gris concreto
        'warm_cozy': 242,      # Beige c√°lido
        'futuristic': 245,     # Blanco tech
        'artistic_bohemian': 238  # Gris art√≠stico
    }
    
    return style_wall_colors.get(style_id, 240)  # Default: gris claro

def get_depth_intensity_for_style(style_id):
    """
    Retorna la intensidad de profundidad apropiada seg√∫n el estilo
    """
    style_depth_intensities = {
        # Estilos normales (profundidad moderada)
        'default': 0.7,
        'casa_ciudad': 0.6,
        'casa_campo': 0.8,
        'casa_playa': 0.5,
        'casa_montana': 0.9,
        'casa_moderna': 0.6,
        
        # Estilos creativos (profundidad variable)
        'minimalist': 0.4,       # Muy sutil
        'luxury': 0.9,           # Muy dram√°tico
        'industrial': 0.8,       # Fuerte
        'warm_cozy': 0.7,        # Moderado
        'futuristic': 0.5,       # Sutil
        'artistic_bohemian': 0.8  # Fuerte
    }
    
    return style_depth_intensities.get(style_id, 0.8)  # Default: 0.8

def log_info(message):
    """Log con timestamp"""
    timestamp = datetime.now().strftime("%H:%M:%S")
    print(f"[{timestamp}] ‚ÑπÔ∏è  {message}")

def log_success(message):
    """Log de √©xito"""
    timestamp = datetime.now().strftime("%H:%M:%S")
    print(f"[{timestamp}] ‚úÖ {message}")

def log_error(message):
    """Log de error"""
    timestamp = datetime.now().strftime("%H:%M:%S")
    print(f"[{timestamp}] ‚ùå {message}")

def log_warning(message):
    """Log de advertencia"""
    timestamp = datetime.now().strftime("%H:%M:%S")
    print(f"[{timestamp}] ‚ö†Ô∏è  {message}")

def calculate_batch_throttle_delay(num_prompts):
    """
    Calcula el tiempo de espera necesario antes de enviar un nuevo lote
    Sin delay entre prompts, solo consideramos el tiempo de procesamiento
    """
    global LAST_BATCH_SUBMIT_TIME, CURRENT_BATCH_PROMPTS
    
    current_time = time.time()
    
    # Tiempo m√≠nimo estimado para que ComfyUI procese el lote anterior
    # Asumimos un m√≠nimo de 1 segundo de procesamiento por prompt
    min_processing_time = CURRENT_BATCH_PROMPTS * 1.0
    estimated_completion_time = LAST_BATCH_SUBMIT_TIME + min_processing_time
    
    # Si ya pas√≥ el tiempo estimado, no hay que esperar
    if current_time >= estimated_completion_time:
        return 0
    
    # Calcular tiempo de espera restante
    wait_time = estimated_completion_time - current_time
    return max(0, wait_time)

def enforce_batch_throttle(num_prompts):
    """
    Aplica el throttling de batches - espera si es necesario
    """
    global LAST_BATCH_SUBMIT_TIME, CURRENT_BATCH_PROMPTS
    
    with BATCH_THROTTLE_LOCK:
        wait_time = calculate_batch_throttle_delay(num_prompts)
        
        if wait_time > 0:
            log_warning(f"üïê Throttling batch: esperando {wait_time:.1f}s para que ComfyUI procese el lote anterior...")
            time.sleep(wait_time)
        
        # Actualizar variables de control
        LAST_BATCH_SUBMIT_TIME = time.time()
        CURRENT_BATCH_PROMPTS = num_prompts
        
        log_info(f"üéØ Batch throttle aplicado: {num_prompts} prompts, env√≠o inmediato")
        
        return wait_time

def allowed_file(filename):
    """Verifica si el archivo tiene una extensi√≥n permitida"""
    if '.' not in filename:
        return False
    ext = filename.rsplit('.', 1)[1].lower()
    return ext in WORKFLOW_CONFIG['allowed_extensions']

def generate_unique_filename(original_filename):
    """Genera un nombre de archivo √∫nico manteniendo la extensi√≥n"""
    if '.' in original_filename:
        name, ext = original_filename.rsplit('.', 1)
        return f"{uuid.uuid4().hex}.{ext.lower()}"
    return f"{uuid.uuid4().hex}.png"

# ==================== GESTI√ìN DE ARCHIVOS ====================

def save_uploaded_image(file, base_name=None):
    """
    Guarda la imagen subida en el directorio de input de ComfyUI
    Retorna: (input_path, filename_for_workflow)
    """
    if not base_name:
        base_name = secure_filename(file.filename.rsplit('.', 1)[0] if '.' in file.filename else 'image')
    
    # Generar nombre √∫nico para evitar conflictos
    unique_filename = f"{base_name}_{uuid.uuid4().hex[:8]}.png"
    input_path = os.path.join(COMFYUI_INPUT_DIR, unique_filename)
    
    try:
        # Abrir y procesar la imagen
        image = Image.open(file.stream)
        
        # Convertir a RGB si es necesario (eliminar canal alpha)
        if image.mode in ('RGBA', 'LA', 'P'):
            # Crear fondo blanco
            background = Image.new('RGB', image.size, (255, 255, 255))
            if image.mode == 'P':
                image = image.convert('RGBA')
            background.paste(image, mask=image.split()[-1] if image.mode in ('RGBA', 'LA') else None)
            image = background
        elif image.mode != 'RGB':
            image = image.convert('RGB')
        
        # Redimensionar si es muy grande (para evitar problemas de memoria)
        max_size = 2048
        if image.width > max_size or image.height > max_size:
            image.thumbnail((max_size, max_size), Image.Resampling.LANCZOS)
            log_info(f"Imagen redimensionada a: {image.width}x{image.height}")
        
        # Guardar como PNG con calidad alta
        image.save(input_path, format='PNG', optimize=False)
        file.stream.seek(0)  # Reset stream para uso posterior
        
        # Establecer permisos de lectura para todos
        try:
            import stat
            os.chmod(input_path, stat.S_IRUSR | stat.S_IWUSR | stat.S_IRGRP | stat.S_IROTH)
        except:
            pass  # No cr√≠tico si falla
        
        # Verificar que el archivo se guard√≥ correctamente
        if os.path.exists(input_path) and os.path.getsize(input_path) > 0:
            file_size = os.path.getsize(input_path)
            log_success(f"Imagen guardada correctamente: {unique_filename} ({file_size} bytes)")
            log_info(f"Ruta completa: {input_path}")
            return input_path, unique_filename
        else:
            raise Exception("El archivo no se guard√≥ correctamente")
        
    except Exception as e:
        log_error(f"Error al guardar imagen: {str(e)}")
        # Limpiar archivo parcial si existe
        if os.path.exists(input_path):
            try:
                os.remove(input_path)
            except:
                pass
        raise

def create_output_directory(base_name):
    """
    Crea directorio de salida para las im√°genes procesadas en NUESTRO directorio personalizado
    Retorna: ruta del directorio creado
    """
    output_dir = os.path.join(OUR_OUTPUT_DIR, base_name)
    os.makedirs(output_dir, exist_ok=True)
    log_info(f"üìÅ Directorio de salida personalizado creado: {output_dir}")
    return output_dir

# ==================== GESTI√ìN DE WORKFLOWS ====================

def load_workflow(workflow_name):
    """
    Carga un workflow desde archivo JSON
    Soporta tanto nombres simples como rutas completas (e.g., "bathroom/H80x60/cuadro-bathroom-open-H60x802")
    Retorna: diccionario del workflow
    """
    # Limpiar el nombre del workflow
    workflow_name = workflow_name.strip()
    
    # Construir posibles rutas
    possible_paths = []
    
    # Si contiene barras, es una ruta relativa completa
    if '/' in workflow_name:
        # Ejemplo: "bathroom/H80x60/cuadro-bathroom-open-H60x802"
        full_path = os.path.join(WORKFLOWS_DIR, workflow_name + '.json')
        possible_paths.append(full_path)
        
        # Tambi√©n probar la ruta directa
        direct_path = os.path.join(WORKFLOWS_DIR, workflow_name)
        possible_paths.append(direct_path)
    
    # Rutas tradicionales (nombre simple)
    possible_paths.extend([
        os.path.join(WORKFLOWS_DIR, f"{workflow_name}.json"),
        os.path.join(WORKFLOWS_DIR, workflow_name),
    ])
    
    # B√∫squeda recursiva como respaldo
    for root, dirs, files in os.walk(WORKFLOWS_DIR):
        for file in files:
            if file.endswith('.json'):
                # Coincidencia exacta del nombre del archivo
                if file == f"{workflow_name}.json" or file == workflow_name:
                    possible_paths.append(os.path.join(root, file))
                
                # Coincidencia del nombre sin extensi√≥n
                file_base = file.replace('.json', '')
                if file_base == workflow_name:
                    possible_paths.append(os.path.join(root, file))
    
    # Buscar el primer archivo que exista
    workflow_path = None
    for path in possible_paths:
        if os.path.exists(path):
            workflow_path = path
            break
    
    if not workflow_path:
        log_error(f"Workflow '{workflow_name}' no encontrado. Rutas intentadas: {possible_paths[:5]}")
        raise FileNotFoundError(f"Workflow '{workflow_name}' no encontrado")
    
    try:
        with open(workflow_path, 'r', encoding='utf-8') as f:
            workflow = json.load(f)
        
        log_success(f"Workflow cargado: {workflow_path}")
        return workflow
        
    except Exception as e:
        log_error(f"Error al cargar workflow: {str(e)}")
        raise

def update_workflow(workflow, image_filename, frame_color='black', style_id='default', style_node_id=None, output_subfolder=None):
    """
    Actualiza el workflow con la nueva imagen, configuraciones y estilo
    
    L√ìGICA IMPLEMENTADA:
    - Por defecto: img2img (m√°s fiel a la imagen original)
    - Con estilo aplicado: text2img + ControlNet depth/canny con strength 0.85
    
    Retorna: workflow actualizado
    """
    workflow_copy = copy.deepcopy(workflow)
    
    # Determinar si se aplica estilo (para decidir img2img vs text2img)
    has_style = style_id and style_id != 'default'
    
    # Verificar si el estilo fuerza text2img
    forces_text2img = False
    if has_style:
        from style_presets import style_forces_text2img
        forces_text2img = style_forces_text2img(style_id)
    
    log_info(f"üéØ Modo de procesamiento: {'TEXT2IMG + ControlNet 0.85 (estilo fuerza)' if forces_text2img else 'IMG2IMG (preservar original)'} (estilo: {style_id})")
    
    # Actualizar nodo LoadImage
    load_node_id = WORKFLOW_CONFIG['load_image_node_id']
    if load_node_id in workflow_copy:
        workflow_copy[load_node_id]['inputs']['image'] = image_filename
        log_success(f"Nodo LoadImage ({load_node_id}) actualizado con: {image_filename}")
    else:
        log_warning(f"Nodo LoadImage ({load_node_id}) no encontrado en el workflow")
    
    # Actualizar nodo DynamicFrameNodeImproved (nodo mejorado con profundidad)
    frame_node_id = WORKFLOW_CONFIG['frame_node_id']
    if frame_node_id in workflow_copy:
        frame_node = workflow_copy[frame_node_id]
        
        # Asegurar que el nodo tiene la estructura inputs
        if 'inputs' not in frame_node:
            frame_node['inputs'] = {}
        
        frame_defaults = WORKFLOW_CONFIG['frame_node_defaults']
        
        # Obtener color de pared, intensidad de profundidad y estilo de perspectiva basado en el estilo
        wall_color = get_wall_color_for_style(style_id)
        depth_intensity = get_depth_intensity_for_style(style_id)
        perspective_style = get_perspective_style_for_style(style_id)
        
        # Configuraci√≥n especial para "none" (sin marco pero con profundidad)
        if frame_color == 'none':
            frame_node['inputs']['preset'] = 'black'  # Usar preset black como base
            frame_node['inputs']['frame_width'] = 0  # Sin marco (ancho 0)
            frame_node['inputs']['depth_enabled'] = True  # Mantener profundidad para efecto 3D
            frame_node['inputs']['depth_intensity'] = depth_intensity
            frame_node['inputs']['perspective_style'] = perspective_style  # Estilo basado en el estilo seleccionado
            frame_node['inputs']['wall_color'] = wall_color
            frame_node['inputs']['upscale_workflow'] = frame_defaults['upscale_workflow']
            
            log_success(f"üö´ DynamicFrameNodeImproved ({frame_node_id}) configurado SIN MARCO pero CON PROFUNDIDAD")
            log_info(f"   üé® Estilo: {style_id} -> Color pared: {wall_color}, Profundidad: {depth_intensity}")
            log_info(f"   üìê Perspectiva: {perspective_style}")
        else:
            # Configuraci√≥n normal con marco y profundidad
            frame_node['inputs']['preset'] = frame_color if frame_color in WORKFLOW_CONFIG['frame_colors'] else 'black'
            frame_node['inputs']['frame_width'] = frame_defaults['frame_width']
            frame_node['inputs']['depth_enabled'] = frame_defaults['depth_enabled']
            frame_node['inputs']['depth_intensity'] = depth_intensity
            frame_node['inputs']['perspective_style'] = perspective_style  # Estilo basado en el estilo seleccionado
            frame_node['inputs']['wall_color'] = wall_color
            frame_node['inputs']['upscale_workflow'] = frame_defaults['upscale_workflow']
            
            # Log detallado para verificar configuraci√≥n
            log_success(f"üñºÔ∏è DynamicFrameNodeImproved ({frame_node_id}) configurado:")
            log_info(f"   üìè Marco: {frame_color} (ancho: {frame_defaults['frame_width']}px)")
            log_info(f"   üé® Preset aplicado: {frame_node['inputs']['preset']}")
            log_info(f"   üèóÔ∏è Profundidad: {'ACTIVADA' if frame_defaults['depth_enabled'] else 'DESACTIVADA'} (intensidad: {depth_intensity})")
            log_info(f"   üìê Perspectiva: {perspective_style}")
            log_info(f"   üéØ Color de pared: {wall_color} (estilo: {style_id})")
            
            # Verificar que el color es v√°lido
            if frame_color not in WORKFLOW_CONFIG['frame_colors']:
                log_warning(f"‚ö†Ô∏è Color '{frame_color}' no est√° en la lista de colores v√°lidos: {WORKFLOW_CONFIG['frame_colors']}")
                log_warning(f"   Usando color por defecto: black")
    else:
        log_warning(f"Nodo DynamicFrameNodeImproved ({frame_node_id}) no encontrado en el workflow")
    
    # CONFIGURAR MODO DE WORKFLOW Y CONTROLNET SEG√öN ESTILO
    if forces_text2img:
        # CON ESTILO QUE FUERZA TEXT2IMG: Cambiar a text2img y activar ControlNet con strength alta
        log_info("üé® Estilo aplicado: Configurando TEXT2IMG + ControlNet 0.85...")
        
        # Cambiar a text2img
        for node_id, node_data in workflow_copy.items():
            if isinstance(node_data, dict) and node_data.get('class_type') == 'SeargeOperatingMode':
                if 'inputs' in node_data and 'workflow_mode' in node_data['inputs']:
                    node_data['inputs']['workflow_mode'] = 'text-to-image'
                    log_success(f"Modo cambiado a TEXT2IMG en nodo {node_id}")
        
        # Configurar ControlNet Depth con strength 0.85
        for node_id, node_data in workflow_copy.items():
            if (isinstance(node_data, dict) and 
                node_data.get('class_type') == 'SeargeControlnetAdapterV2' and
                'inputs' in node_data and 
                node_data['inputs'].get('controlnet_mode') == 'depth'):
                
                node_data['inputs']['strength'] = 0.85
                log_success(f"ControlNet Depth strength actualizado a 0.85 en nodo {node_id}")
        
        # Configurar ControlNet Canny con strength 0.85
        for node_id, node_data in workflow_copy.items():
            if (isinstance(node_data, dict) and 
                node_data.get('class_type') == 'SeargeControlnetAdapterV2' and
                'inputs' in node_data and 
                node_data['inputs'].get('controlnet_mode') == 'canny'):
                
                node_data['inputs']['strength'] = 0.85
                log_success(f"ControlNet Canny strength actualizado a 0.85 en nodo {node_id}")
        
        # Aplicar el estilo
        from style_presets import get_style_prompt, get_style_negative_prompt, style_forces_text2img
        style_prompt = get_style_prompt(style_id)
        negative_prompt = get_style_negative_prompt(style_id)
        log_info(f"üìù Prompt del estilo: '{style_prompt[:100]}...'")
        if negative_prompt:
            log_info(f"‚ùå Prompt negativo del estilo: '{negative_prompt[:100]}...'")
        
        workflow_copy = apply_style_to_workflow(workflow_copy, style_id, style_node_id)
        
        # Verificar que se aplic√≥ correctamente
        if style_node_id:
            if style_node_id in workflow_copy:
                applied_value = ""
                node = workflow_copy[style_node_id]
                if "inputs" in node:
                    applied_value = node["inputs"].get("prompt", node["inputs"].get("text", ""))
                log_success(f"Estilo aplicado: {style_id} al nodo {style_node_id}")
                log_info(f"Valor aplicado en nodo {style_node_id}: '{applied_value}'")
            else:
                log_error(f"Nodo especificado {style_node_id} no existe en el workflow")
        else:
            # Verificar en el nodo de estilo por defecto para Searge
            style_node_6 = workflow_copy.get("6", {})
            if "inputs" in style_node_6:
                applied_value = style_node_6["inputs"].get("prompt", "")
                log_success(f"Estilo aplicado: {style_id} (auto-detectado al nodo 6)")
                log_info(f"Valor aplicado en nodo 6: '{applied_value}'")
            else:
                log_warning("No se pudo verificar la aplicaci√≥n del estilo")
    
    else:
        # SIN ESTILO O ESTILO QUE NO FUERZA TEXT2IMG: Asegurar img2img y ControlNet con strength baja
        log_info("üì∑ Sin estilo o estilo compatible: Manteniendo IMG2IMG...")
        
        # Asegurar img2img
        for node_id, node_data in workflow_copy.items():
            if isinstance(node_data, dict) and node_data.get('class_type') == 'SeargeOperatingMode':
                if 'inputs' in node_data and 'workflow_mode' in node_data['inputs']:
                    node_data['inputs']['workflow_mode'] = 'image-to-image'
                    log_success(f"Modo mantenido en IMG2IMG en nodo {node_id}")
        
        # Configurar ControlNet con strength m√°s baja para preservar imagen original
        for node_id, node_data in workflow_copy.items():
            if (isinstance(node_data, dict) and 
                node_data.get('class_type') == 'SeargeControlnetAdapterV2' and
                'inputs' in node_data and 
                node_data['inputs'].get('controlnet_mode') == 'depth'):
                
                node_data['inputs']['strength'] = 0.2  # Strength baja para img2img
                log_success(f"ControlNet Depth strength mantenido en 0.2 para IMG2IMG en nodo {node_id}")
        
        for node_id, node_data in workflow_copy.items():
            if (isinstance(node_data, dict) and 
                node_data.get('class_type') == 'SeargeControlnetAdapterV2' and
                'inputs' in node_data and 
                node_data['inputs'].get('controlnet_mode') == 'canny'):
                
                node_data['inputs']['strength'] = 0.71  # Strength actual para img2img
                log_success(f"ControlNet Canny strength mantenido en 0.71 para IMG2IMG en nodo {node_id}")
    
    # Actualizar nodos SaveImage (subfolder si se especifica Y configurar prefijo descriptivo)
    save_node_id = WORKFLOW_CONFIG['save_image_node_id']
    
    # NO actualizar el prefijo del nodo principal (704) para mantener "ComfyUI" 
    # Esto permite identificar correctamente la imagen de composici√≥n final
    # if save_node_id in workflow_copy:
    #     save_node = workflow_copy[save_node_id]
    #     if 'inputs' in save_node:
    #         # Crear prefijo basado en la imagen y frame_color
    #         base_image_name = image_filename.split('.')[0] if '.' in image_filename else image_filename
    #         descriptive_prefix = f"{base_image_name}_{frame_color}"
    #         save_node['inputs']['filename_prefix'] = descriptive_prefix
    #         log_success(f"SaveImage {save_node_id}: prefijo actualizado a '{descriptive_prefix}'")
    
    log_info(f"üíæ Manteniendo prefijo original 'ComfyUI' en nodo {save_node_id} para identificaci√≥n correcta")
    
    if output_subfolder:
        for node_id, node_data in workflow_copy.items():
            if isinstance(node_data, dict) and node_data.get('class_type') == 'SaveImage':
                if 'inputs' in node_data and 'filename_prefix' in node_data['inputs']:
                    old_prefix = node_data['inputs']['filename_prefix']
                    new_prefix = f"{output_subfolder}/{old_prefix}"
                    node_data['inputs']['filename_prefix'] = new_prefix
                    log_info(f"SaveImage {node_id}: {old_prefix} ‚Üí {new_prefix}")
    
    # Randomizar seeds
    for node_id, node_data in workflow_copy.items():
        if isinstance(node_data, dict) and 'inputs' in node_data and 'seed' in node_data['inputs']:
            new_seed = random.randint(1, 2**32-1)
            node_data['inputs']['seed'] = new_seed
    
    log_success(f"‚úÖ Workflow actualizado correctamente en modo: {'TEXT2IMG + ControlNet 0.85' if forces_text2img else 'IMG2IMG (fiel al original)'}")
    return workflow_copy

# ==================== COMUNICACI√ìN CON COMFYUI ====================

def verify_image_accessibility(filename):
    """
    Verifica que ComfyUI pueda acceder al archivo de imagen
    Retorna: True si es accesible, False en caso contrario
    """
    try:
        # Verificar que el archivo existe f√≠sicamente
        input_path = os.path.join(COMFYUI_INPUT_DIR, filename)
        if not os.path.exists(input_path):
            log_error(f"Archivo no existe en disco: {input_path}")
            return False
        
        file_size = os.path.getsize(input_path)
        if file_size == 0:
            log_error(f"Archivo vac√≠o: {input_path}")
            return False
        
        log_info(f"Archivo verificado en disco: {filename} ({file_size} bytes)")
        
        # Hacer una petici√≥n a ComfyUI para verificar la imagen
        response = requests.get(f"{COMFYUI_URL}/view", params={'filename': filename}, timeout=10)
        if response.status_code == 200:
            log_success(f"Imagen accesible para ComfyUI: {filename}")
            return True
        else:
            log_warning(f"ComfyUI no puede acceder a la imagen: {filename} (status: {response.status_code})")
            
            # Intentar con diferentes par√°metros
            response2 = requests.get(f"{COMFYUI_URL}/view", params={'filename': filename, 'type': 'input'}, timeout=10)
            if response2.status_code == 200:
                log_success(f"Imagen accesible para ComfyUI (con type=input): {filename}")
                return True
            
            return False
    except Exception as e:
        log_warning(f"Error verificando acceso a imagen: {str(e)}")
        return False

def submit_workflow_to_comfyui(workflow):
    """
    Env√≠a el workflow a ComfyUI para procesamiento
    Retorna: prompt_id
    """
    client_id = str(uuid.uuid4())
    prompt_data = {
        "prompt": workflow,
        "client_id": client_id
    }
    
    try:
        response = requests.post(f"{COMFYUI_URL}/prompt", json=prompt_data, timeout=60)
        response.raise_for_status()
        
        result = response.json()
        prompt_id = result.get('prompt_id')
        
        if not prompt_id:
            raise ValueError("ComfyUI no devolvi√≥ prompt_id")
        
        log_success(f"Workflow enviado a ComfyUI. Prompt ID: {prompt_id}")
        return prompt_id
        
    except Exception as e:
        log_error(f"Error enviando workflow a ComfyUI: {str(e)}")
        raise

def wait_for_completion(prompt_id, timeout=300):
    """
    Espera a que ComfyUI complete el procesamiento
    Retorna: outputs del workflow
    """
    log_info(f"Esperando completion del prompt: {prompt_id}")
    
    for i in range(timeout):
        try:
            response = requests.get(f"{COMFYUI_URL}/history/{prompt_id}", timeout=30)
            
            if response.status_code == 200:
                history = response.json()
                
                if prompt_id in history:
                    prompt_history = history[prompt_id]
                    
                    # Verificar si hay outputs
                    if 'outputs' in prompt_history:
                        log_success("Procesamiento completado")
                        return prompt_history['outputs']
                    
                    # Verificar errores
                    if 'status' in prompt_history and 'error' in prompt_history['status']:
                        error_msg = prompt_history['status']['error']
                        raise Exception(f"Error en ComfyUI: {error_msg}")
            
            if i % 10 == 0 and i > 0:
                log_info(f"Esperando... {i}/{timeout}s")
            
            time.sleep(1)
            
        except requests.exceptions.RequestException:
            time.sleep(1)
            continue
    
    raise TimeoutError(f"Timeout esperando completion despu√©s de {timeout} segundos")

# ==================== PROCESAMIENTO DE RESULTADOS ====================

def extract_generated_images(outputs, original_image_filename=None, include_upscale=True):
    """
    Extrae las im√°genes GENERADAS de los outputs de ComfyUI para mostrar en el frontend:
    1. Upscale (imagen escalada del nodo 696) - OPCIONAL seg√∫n include_upscale
    2. Composici√≥n (imagen final combinada del nodo 704) - SIEMPRE incluida
    
    NOTA: La imagen original NO se incluye en el resultado para evitar mostrarla en el frontend.
    
    Args:
        outputs: Outputs de ComfyUI
        original_image_filename: Nombre del archivo original para mejor clasificaci√≥n
        include_upscale: Si True incluye imagen upscale, si False solo composici√≥n
    
    Retorna: lista de im√°genes generadas filtradas (composici√≥n + upscale opcional)
    """
    # Contenedores para los 3 tipos de im√°genes
    original_image = None
    upscale_image = None
    composition_image = None
    
    save_node_id = WORKFLOW_CONFIG['save_image_node_id']
    
    def classify_image_type(filename):
        """Clasifica el tipo de imagen basado en el nombre del archivo"""
        filename_lower = filename.lower()
        
        # Excluir archivos temporales
        temporal_patterns = ['tmp_', 'temp_', '_temp']
        for pattern in temporal_patterns:
            if pattern in filename_lower:
                return None
        
        # Clasificar tipos espec√≠ficos CORREGIDOS:
        # 1. COMPOSICI√ìN FINAL: Im√°genes que empiezan con "comfyui" (nodo 704)
        if filename_lower.startswith('comfyui') and any(char.isdigit() for char in filename_lower):
            return 'composition'  # ‚úÖ Resultado final del workflow (nodo 704)
        # 2. UPSCALE ORIGINAL: Im√°genes con "original-upscale" (nodo 696)
        elif 'original-upscale' in filename_lower:
            return 'upscale'  # ‚úÖ Solo upscale de la imagen original (nodo 696)
        # 3. UPSCALE GEN√âRICO: Otras im√°genes con "upscale" pero sin "original"
        elif 'upscale' in filename_lower and 'original' not in filename_lower:
            return 'upscale'  # Solo upscale
        # 4. ORIGINAL: Im√°genes con "original" pero sin "upscale"
        elif 'original' in filename_lower and 'upscale' not in filename_lower:
            return 'original'  # Solo original
        else:
            # Usar keywords de habitaciones como fallback para original
            room_keywords = ['bathroom', 'bedroom', 'office', 'salon', 'kitchen', 'living']
            if any(keyword in filename_lower for keyword in room_keywords):
                return 'original'
        
        return None
    
    def add_image_if_better(current_image, new_image_info, image_type):
        """A√±ade la imagen si es mejor que la actual o si no hay una actual"""
        if current_image is None:
            log_info(f"‚úÖ Primera imagen {image_type}: {new_image_info['filename']} (nodo {new_image_info['node_id']})")
            return new_image_info
        
        # üéØ PRIORIDAD M√ÅXIMA: Nodo 704 siempre gana para composici√≥n
        if image_type == 'composition':
            if new_image_info['node_id'] == '704' and current_image['node_id'] != '704':
                log_info(f"‚úÖ Mejor imagen {image_type} (nodo 704 - composici√≥n final): {new_image_info['filename']}")
                return new_image_info
            elif current_image['node_id'] == '704' and new_image_info['node_id'] != '704':
                log_info(f"‚ö†Ô∏è Manteniendo imagen {image_type} (nodo 704 prioritario): {current_image['filename']}")
                return current_image
        
        # Preferir im√°genes que contengan el nombre original
        if original_image_filename:
            original_base = original_image_filename.split('.')[0].lower() if '.' in original_image_filename else original_image_filename.lower()
            current_has_original = original_base in current_image['filename'].lower()
            new_has_original = original_base in new_image_info['filename'].lower()
            
            if new_has_original and not current_has_original:
                log_info(f"‚úÖ Mejor imagen {image_type} (contiene nombre original): {new_image_info['filename']}")
                return new_image_info
            elif current_has_original and not new_has_original:
                log_info(f"‚ö†Ô∏è Manteniendo imagen {image_type} actual (contiene nombre original): {current_image['filename']}")
                return current_image
        
        # Si ambas son similares, preferir la m√°s reciente (por nombre)
        if new_image_info['filename'] > current_image['filename']:
            log_info(f"‚úÖ Mejor imagen {image_type} (m√°s reciente): {new_image_info['filename']}")
            return new_image_info
        
        log_info(f"‚ö†Ô∏è Manteniendo imagen {image_type} actual: {current_image['filename']}")
        return current_image
    
    # Buscar en todos los nodos SaveImage (704 primero para prioridad de composici√≥n)
    all_save_nodes = ['704', save_node_id, '696']  # 704 primero (composici√≥n final), luego los dem√°s
    
    for node_id in all_save_nodes:
        if node_id in outputs and isinstance(outputs[node_id], dict) and 'images' in outputs[node_id]:
            for image_info in outputs[node_id]['images']:
                if 'filename' not in image_info:
                    continue
                
                filename = image_info['filename']
                image_type = classify_image_type(filename)
                
                log_info(f"üîç Analizando imagen: {filename} (nodo {node_id}) -> clasificada como: {image_type}")
                
                if image_type is None:
                    log_info(f"‚ùå Archivo excluido (no clasificado): {filename}")
                    continue
                
                # Crear info de imagen
                img_info = {
                    'filename': filename,
                    'subfolder': image_info.get('subfolder', ''),
                    'type': image_info.get('type', 'output'),
                    'node_id': node_id,
                    'image_type': image_type
                }
                
                # üéØ CLASIFICACI√ìN ADICIONAL POR NODO (m√°s confiable que el nombre)
                if node_id == '704':
                    img_info['image_type'] = 'composition'  # Nodo 704 siempre es composici√≥n final
                    log_info(f"üéØ FORZADO: Imagen del nodo 704 clasificada como COMPOSITION: {filename}")
                elif node_id == '696':
                    img_info['image_type'] = 'upscale'  # Nodo 696 siempre es upscale original
                    log_info(f"üìà FORZADO: Imagen del nodo 696 clasificada como UPSCALE: {filename}")
                
                # Usar la clasificaci√≥n forzada por nodo
                final_image_type = img_info['image_type']
                
                # Asignar a la categor√≠a correspondiente usando clasificaci√≥n forzada por nodo
                if final_image_type == 'original':
                    original_image = add_image_if_better(original_image, img_info, 'original')
                elif final_image_type == 'upscale':
                    upscale_image = add_image_if_better(upscale_image, img_info, 'upscale')
                elif final_image_type == 'composition':
                    composition_image = add_image_if_better(composition_image, img_info, 'composition')
    
    # Buscar en otros nodos si no se encontraron todas las im√°genes
    if not all([original_image, upscale_image, composition_image]):
        log_info("üîç Buscando im√°genes faltantes en otros nodos...")
        
        for node_id, node_data in outputs.items():
            if node_id in all_save_nodes:
                continue  # Ya procesado
            
            if isinstance(node_data, dict) and 'images' in node_data:
                for image_info in node_data['images']:
                    if 'filename' not in image_info:
                        continue
                    
                    filename = image_info['filename']
                    image_type = classify_image_type(filename)
                    
                    if image_type is None:
                        continue
                    
                    img_info = {
                        'filename': filename,
                        'subfolder': image_info.get('subfolder', ''),
                        'type': image_info.get('type', 'output'),
                        'node_id': node_id,
                        'image_type': image_type
                    }
                    
                    # Solo asignar si no tenemos una imagen de ese tipo
                    if image_type == 'original' and original_image is None:
                        original_image = img_info
                        log_info(f"‚úÖ Imagen original encontrada en nodo {node_id}: {filename}")
                    elif image_type == 'upscale' and upscale_image is None:
                        upscale_image = img_info
                        log_info(f"‚úÖ Imagen upscale encontrada en nodo {node_id}: {filename}")
                    elif image_type == 'composition' and composition_image is None:
                        composition_image = img_info
                        log_info(f"‚úÖ Imagen composici√≥n encontrada en nodo {node_id}: {filename}")
    
    # Ensamblar resultado final - SOLO IM√ÅGENES GENERADAS (sin la original)
    final_images = []
    
    # üéØ COMPOSICI√ìN: Siempre incluir la imagen de composici√≥n (nodo 704)
    if composition_image:
        final_images.append(composition_image)
    
    # üìà UPSCALE: Incluir solo si el usuario lo solicita
    if include_upscale and upscale_image:
        final_images.append(upscale_image)
    
    # Calcular im√°genes esperadas seg√∫n configuraci√≥n
    expected_generated = 1 + (1 if include_upscale else 0)  # Composici√≥n + upscale opcional
    if len(final_images) != expected_generated:
        log_warning(f"‚ö†Ô∏è Se esperaban {expected_generated} im√°genes generadas, pero se encontraron {len(final_images)}")
        log_warning(f"   Upscale: {'‚úÖ' if upscale_image else '‚ùå'} {'(incluido)' if include_upscale else '(excluido por usuario)'}")
        log_warning(f"   Composici√≥n: {'‚úÖ' if composition_image else '‚ùå'}")
        if original_image:
            log_info(f"   Original encontrada pero excluida del frontend: {original_image['filename']}")
    
    log_success(f"‚úÖ Total de im√°genes generadas para frontend: {len(final_images)}/{expected_generated}")
    log_info(f"üìã RESUMEN DE IM√ÅGENES GENERADAS PARA FRONTEND:")
    for img in final_images:
        log_info(f"   üì∑ {img['image_type'].upper()}: {img['filename']} (nodo {img['node_id']})")
    
    # Log especial para indicar estado del upscale
    if upscale_image:
        if include_upscale:
            log_info(f"üìà IMAGEN UPSCALE INCLUIDA EN FRONTEND Y GUARDADO: {upscale_image['filename']} (nodo {upscale_image['node_id']})")
        else:
            log_info(f"üö´ IMAGEN UPSCALE EXCLUIDA DEL FRONTEND Y GUARDADO: {upscale_image['filename']} (nodo {upscale_image['node_id']})")
    else:
        log_warning(f"‚ö†Ô∏è No se encontr√≥ imagen upscale en los outputs de ComfyUI")
    
    # Log especial para indicar que la original se excluye
    if original_image:
        log_info(f"üö´ IMAGEN ORIGINAL EXCLUIDA DEL FRONTEND: {original_image['filename']} (nodo {original_image['node_id']})")
    
    # Log especial para composici√≥n final
    if composition_image:
        log_success(f"üéØ IMAGEN DE COMPOSICI√ìN FINAL (nodo 704): {composition_image['filename']}")
    else:
        log_error(f"‚ùå NO SE ENCONTR√ì IMAGEN DE COMPOSICI√ìN FINAL (nodo 704)")
    
    return final_images

def find_image_file(filename, subfolder=''):
    """
    Busca un archivo de imagen en el directorio de output DE COMFYUI (para leer las im√°genes generadas)
    NOTA: ComfyUI siempre guardar√° todas las im√°genes aqu√≠, nosotros solo leemos de aqu√≠
    Retorna: ruta completa del archivo o None
    """
    possible_paths = [
        os.path.join(COMFYUI_OUTPUT_DIR, subfolder, filename) if subfolder else os.path.join(COMFYUI_OUTPUT_DIR, filename),
        os.path.join(COMFYUI_OUTPUT_DIR, filename)
    ]
    
    # B√∫squeda recursiva como fallback
    for root, dirs, files in os.walk(COMFYUI_OUTPUT_DIR):
        if filename in files:
            possible_paths.append(os.path.join(root, filename))
    
    for path in possible_paths:
        if os.path.exists(path):
            log_success(f"üìñ Imagen encontrada en ComfyUI output: {path}")
            return path
    
    log_error(f"‚ùå Imagen no encontrada en ComfyUI output: {filename}")
    return None

def save_images_to_our_output(output_dir, original_file, generated_images, original_filename, include_upscale, job_id, workflow_name=None, style_id=None):
    """
    Guarda las im√°genes en nuestro directorio personalizado de manera organizada
    
    Args:
        output_dir: Directorio de salida personalizado para esta imagen
        original_file: Archivo original subido
        generated_images: Lista de im√°genes generadas (ya filtradas)
        original_filename: Nombre del archivo original
        include_upscale: Si incluir upscale en el guardado
        job_id: ID del trabajo de sesi√≥n
        workflow_name: Nombre del workflow utilizado (para nombres descriptivos)
        style_id: ID del estilo aplicado (para nombres descriptivos)
    
    Returns:
        (original_info, saved_images) - Informaci√≥n de imagen original y lista de generadas guardadas
    """
    log_info(f"üíæ Guardando en directorio personalizado: {output_dir}")
    
    saved_images = []
    
    # 1. üì∑ GUARDAR IMAGEN ORIGINAL (convertida a JPG con optimizaci√≥n)
    log_info("üì∑ Guardando imagen original...")
    original_filename_clean = secure_filename(original_filename.rsplit('.', 1)[0] if '.' in original_filename else 'image')
    original_ext = original_filename.rsplit('.', 1)[1] if '.' in original_filename else 'png'
    
    # Convertir imagen original a JPG con optimizaci√≥n
    original_dest_filename = f"original.jpg"  # Siempre JPG
    original_dest_path = os.path.join(output_dir, original_dest_filename)
    
    try:
        # Guardar imagen original desde el archivo subido
        original_file.stream.seek(0)  # Reset stream
        image = Image.open(original_file.stream)
        original_size_kb = len(original_file.stream.read()) / 1024
        original_file.stream.seek(0)  # Reset stream again
        
        # Convertir a RGB si es necesario
        if image.mode in ('RGBA', 'LA', 'P'):
            background = Image.new('RGB', image.size, (255, 255, 255))
            if image.mode == 'P':
                image = image.convert('RGBA')
            background.paste(image, mask=image.split()[-1] if image.mode in ('RGBA', 'LA') else None)
            image = background
        elif image.mode != 'RGB':
            image = image.convert('RGB')
        
        # Optimizar imagen original a JPG con l√≠mite de 200KB
        target_size_kb = 200
        
        # Intentar calidad 100% primero
        buffer = io.BytesIO()
        image.save(buffer, format='JPEG', quality=100, optimize=True)
        size_kb = buffer.tell() / 1024
        
        if size_kb <= target_size_kb:
            # Perfecto con calidad 100%
            with open(original_dest_path, 'wb') as f:
                f.write(buffer.getvalue())
            log_success(f"‚úÖ Imagen original convertida a JPG con calidad 100%: {original_dest_filename} ({size_kb:.1f}KB)")
            log_info(f"   üìä Reducci√≥n de tama√±o: {original_size_kb:.1f}KB ‚Üí {size_kb:.1f}KB ({((original_size_kb - size_kb) / original_size_kb * 100):.1f}% reducci√≥n)")
        else:
            # Optimizar gradualmente para mantener la mejor calidad posible
            log_info(f"üìè Imagen original muy grande con calidad 100% ({size_kb:.1f}KB), optimizando para 200KB...")
            
            best_quality = 100
            best_buffer = None
            
            # Optimizaci√≥n gradual m√°s inteligente
            for quality in range(95, 60, -2):  # Reducir de 2 en 2 desde 95% hasta 60%
                buffer = io.BytesIO()
                image.save(buffer, format='JPEG', quality=quality, optimize=True)
                size_kb = buffer.tell() / 1024
                
                if size_kb <= target_size_kb:
                    best_quality = quality
                    best_buffer = buffer.getvalue()
                    break
            
            # Si a√∫n no encontramos una buena calidad, probar con pasos m√°s grandes
            if best_buffer is None:
                for quality in range(60, 30, -5):  # Reducir de 5 en 5 desde 60% hasta 30%
                    buffer = io.BytesIO()
                    image.save(buffer, format='JPEG', quality=quality, optimize=True)
                    size_kb = buffer.tell() / 1024
                    
                    if size_kb <= target_size_kb:
                        best_quality = quality
                        best_buffer = buffer.getvalue()
                        break
            
            # Guardar la mejor versi√≥n encontrada
            if best_buffer:
                with open(original_dest_path, 'wb') as f:
                    f.write(best_buffer)
                final_size = len(best_buffer) / 1024
                log_info(f"‚úÖ Imagen original convertida a JPG: {original_dest_filename} ({final_size:.1f}KB, calidad {best_quality}%)")
                log_info(f"   üìä Reducci√≥n de tama√±o: {original_size_kb:.1f}KB ‚Üí {final_size:.1f}KB ({((original_size_kb - final_size) / original_size_kb * 100):.1f}% reducci√≥n)")
            else:
                # Como √∫ltimo recurso, guardar con calidad 30%
                image.save(original_dest_path, format='JPEG', quality=30, optimize=True)
                final_size = os.path.getsize(original_dest_path) / 1024
                log_warning(f"‚ö†Ô∏è Imagen original muy grande, guardada con calidad 30%: {original_dest_filename} ({final_size:.1f}KB)")
                log_info(f"   üìä Reducci√≥n de tama√±o: {original_size_kb:.1f}KB ‚Üí {final_size:.1f}KB ({((original_size_kb - final_size) / original_size_kb * 100):.1f}% reducci√≥n)")
        
        # Guardar tambi√©n en sesi√≥n
        with open(original_dest_path, 'rb') as img_file:
            session_original_url = session_manager.save_job_image(job_id, img_file.read(), original_dest_filename)
        
        original_info = {
            'filename': original_dest_filename,
            'url': f"/get-image/{os.path.basename(output_dir)}/{original_dest_filename}",
            'session_url': session_original_url,
            'image_type': 'original',
            'status': 'saved'
        }
        
        log_success(f"‚úÖ Imagen original guardada: {original_dest_path}")
        
    except Exception as e:
        log_error(f"‚ùå Error guardando imagen original: {str(e)}")
        original_info = {
            'filename': original_dest_filename,
            'error': str(e),
            'status': 'error'
        }
    
    # 2. üéØ GUARDAR IM√ÅGENES GENERADAS (ya filtradas seg√∫n include_upscale)
    log_info(f"üéØ Guardando {len(generated_images)} im√°genes generadas...")
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # üîç VERIFICAR ARCHIVOS EXISTENTES EN EL DIRECTORIO
    existing_files = []
    if os.path.exists(output_dir):
        existing_files = [f.lower() for f in os.listdir(output_dir)]
    
    # Verificar si ya existe una imagen upscale
    existing_upscale = any('upscale_' in f for f in existing_files)
    if existing_upscale:
        log_info(f"üìà Ya existe una imagen upscale en {output_dir}, se omitir√°n nuevas im√°genes upscale")
    
    for i, img_info in enumerate(generated_images):
        try:
            # Verificar tipo de imagen
            img_type = img_info.get('image_type', 'generated')
            
            # üö´ SALTAR UPSCALE SI YA EXISTE UNA
            if img_type == 'upscale' and existing_upscale:
                log_warning(f"‚ö†Ô∏è Imagen upscale saltada (ya existe una): {img_info['filename']}")
                # Buscar la imagen upscale existente para agregarla a la respuesta
                for existing_file in os.listdir(output_dir):
                    if 'upscale_' in existing_file.lower():
                        # Crear entrada para la imagen existente
                        with open(os.path.join(output_dir, existing_file), 'rb') as img_file:
                            session_url = session_manager.save_job_image(job_id, img_file.read(), existing_file)
                        
                        saved_images.append({
                            'filename': existing_file,
                            'url': f"/get-image/{os.path.basename(output_dir)}/{existing_file}",
                            'session_url': session_url,
                            'original_filename': img_info['filename'],
                            'node_id': img_info.get('node_id'),
                            'image_type': img_type,
                            'status': 'existing'  # Marcar como existente
                        })
                        log_info(f"üìà Usando imagen upscale existente: {existing_file}")
                        break
                continue
            
            # Buscar archivo fuente en ComfyUI output
            source_path = find_image_file(img_info['filename'], img_info['subfolder'])
            if not source_path:
                log_error(f"‚ùå No se pudo encontrar: {img_info['filename']}")
                continue
            
            # Crear nombre descriptivo
            original_ext = img_info['filename'].rsplit('.', 1)[1] if '.' in img_info['filename'] else 'png'
            
            # Preparar componentes para el nombre del archivo
            workflow_clean = workflow_name.replace('/', '_').replace('-', '_') if workflow_name else 'workflow'
            style_clean = style_id if style_id and style_id != 'default' else 'nostyle'
            photo_name = secure_filename(original_filename.rsplit('.', 1)[0] if '.' in original_filename else 'image')
            
            # Nombres descriptivos seg√∫n tipo
            if img_type == 'composition':
                # Formato: workflow_estilo_nombreFoto_timestamp.ext
                dest_filename = f"{workflow_clean}_{style_clean}_{photo_name}_{timestamp}.{original_ext}"
                log_info(f"üéØ Nombre de composici√≥n generado: {dest_filename}")
                log_info(f"   üìù Componentes: workflow={workflow_clean}, estilo={style_clean}, foto={photo_name}, timestamp={timestamp}")
            elif img_type == 'upscale':
                # Formato: upscale_nombreFoto_timestamp.ext (m√°s simple para upscale)
                dest_filename = f"upscale_{photo_name}_{timestamp}.{original_ext}"
                log_info(f"üìà Nombre de upscale generado: {dest_filename}")
                # Marcar que ahora tenemos upscale para futuras verificaciones
                existing_upscale = True
            else:
                # Formato gen√©rico: generated_numeroSecuencia_timestamp.ext
                dest_filename = f"generated_{i+1}_{timestamp}.{original_ext}"
            
            dest_path = os.path.join(output_dir, dest_filename)
            
            # Verificar si ya existe este archivo espec√≠fico
            if os.path.exists(dest_path):
                log_warning(f"‚ö†Ô∏è Archivo espec√≠fico ya existe, saltando: {dest_filename}")
                continue
            
            # Convertir PNG a JPG con l√≠mite de 200KB manteniendo m√°xima calidad
            if original_ext.lower() == 'png':
                # Cambiar extensi√≥n a JPG
                dest_filename = dest_filename.replace('.png', '.jpg')
                dest_path = os.path.join(output_dir, dest_filename)
                
                # Convertir PNG a JPG con optimizaci√≥n inteligente de tama√±o
                try:
                    from PIL import Image
                    import io
                    
                    # Abrir imagen PNG original
                    img = Image.open(source_path)
                    original_size_kb = os.path.getsize(source_path) / 1024
                    
                    # Convertir a RGB si es necesario (PNG puede tener transparencia)
                    if img.mode in ('RGBA', 'LA', 'P'):
                        # Crear fondo blanco para im√°genes con transparencia
                        background = Image.new('RGB', img.size, (255, 255, 255))
                        if img.mode == 'P':
                            img = img.convert('RGBA')
                        background.paste(img, mask=img.split()[-1] if img.mode == 'RGBA' else None)
                        img = background
                    elif img.mode != 'RGB':
                        img = img.convert('RGB')
                    
                    # Estrategia de optimizaci√≥n: Comenzar con calidad 100% y reducir gradualmente
                    target_size_kb = 200
                    best_quality = 100
                    best_buffer = None
                    
                    # Probar calidad 100% primero
                    buffer = io.BytesIO()
                    img.save(buffer, format='JPEG', quality=100, optimize=True)
                    size_kb = buffer.tell() / 1024
                    
                    if size_kb <= target_size_kb:
                        # ¬°Perfecto! Calidad 100% y dentro del l√≠mite
                        with open(dest_path, 'wb') as f:
                            f.write(buffer.getvalue())
                        log_success(f"üéØ Imagen convertida a JPG con calidad 100%: {dest_filename} ({size_kb:.1f}KB)")
                        log_info(f"   üìä Reducci√≥n de tama√±o: {original_size_kb:.1f}KB PNG ‚Üí {size_kb:.1f}KB JPG ({((original_size_kb - size_kb) / original_size_kb * 100):.1f}% reducci√≥n)")
                    else:
                        # Necesitamos reducir el tama√±o, pero manteniendo la mejor calidad posible
                        log_info(f"üìè Imagen muy grande con calidad 100% ({size_kb:.1f}KB), optimizando para 200KB...")
                        
                        # Algoritmo de optimizaci√≥n m√°s inteligente: reducir en pasos m√°s peque√±os
                        for quality in range(95, 60, -2):  # Reducir de 2 en 2 desde 95% hasta 60%
                            buffer = io.BytesIO()
                            img.save(buffer, format='JPEG', quality=quality, optimize=True)
                            size_kb = buffer.tell() / 1024
                            
                            if size_kb <= target_size_kb:
                                best_quality = quality
                                best_buffer = buffer.getvalue()
                                break
                        
                        # Si a√∫n no encontramos una buena calidad, probar con pasos m√°s grandes
                        if best_buffer is None:
                            log_warning(f"‚ö†Ô∏è Imagen muy grande, probando calidades m√°s bajas...")
                            for quality in range(60, 30, -5):  # Reducir de 5 en 5 desde 60% hasta 30%
                                buffer = io.BytesIO()
                                img.save(buffer, format='JPEG', quality=quality, optimize=True)
                                size_kb = buffer.tell() / 1024
                                
                                if size_kb <= target_size_kb:
                                    best_quality = quality
                                    best_buffer = buffer.getvalue()
                                    break
                        
                        # Guardar la mejor versi√≥n encontrada
                        if best_buffer:
                            with open(dest_path, 'wb') as f:
                                f.write(best_buffer)
                            final_size = len(best_buffer) / 1024
                            log_info(f"üéØ Imagen convertida a JPG: {dest_filename} ({final_size:.1f}KB, calidad {best_quality}%)")
                            log_info(f"   üìä Reducci√≥n de tama√±o: {original_size_kb:.1f}KB PNG ‚Üí {final_size:.1f}KB JPG ({((original_size_kb - final_size) / original_size_kb * 100):.1f}% reducci√≥n)")
                        else:
                            # Como √∫ltimo recurso, guardar con calidad 30%
                            img.save(dest_path, format='JPEG', quality=30, optimize=True)
                            final_size = os.path.getsize(dest_path) / 1024
                            log_warning(f"‚ö†Ô∏è Imagen muy grande, guardada con calidad 30%: {dest_filename} ({final_size:.1f}KB)")
                            log_info(f"   üìä Reducci√≥n de tama√±o: {original_size_kb:.1f}KB PNG ‚Üí {final_size:.1f}KB JPG ({((original_size_kb - final_size) / original_size_kb * 100):.1f}% reducci√≥n)")
                    
                except Exception as e:
                    log_error(f"‚ùå Error convirtiendo PNG a JPG: {str(e)}")
                    # Fallback: copiar archivo original
                    shutil.copy2(source_path, dest_path)
            else:
                # Copiar archivo no-PNG normalmente
                shutil.copy2(source_path, dest_path)
            
            # Guardar tambi√©n en sesi√≥n
            with open(dest_path, 'rb') as img_file:
                session_url = session_manager.save_job_image(job_id, img_file.read(), dest_filename)
            
            # Agregar a lista de guardadas
            saved_image_info = {
                'filename': dest_filename,
                'url': f"/get-image/{os.path.basename(output_dir)}/{dest_filename}",
                'session_url': session_url,
                'original_filename': img_info['filename'],
                'node_id': img_info.get('node_id'),
                'image_type': img_type,
                'status': 'saved'
            }
            
            saved_images.append(saved_image_info)
            log_success(f"‚úÖ {img_type.upper()} guardada: {dest_path}")
            
        except Exception as e:
            log_error(f"‚ùå Error guardando imagen {img_info['filename']}: {str(e)}")
            # Agregar entrada de error
            saved_images.append({
                'filename': img_info['filename'],
                'error': str(e),
                'image_type': img_info.get('image_type', 'unknown'),
                'status': 'error'
            })
    
    # 3. üìä RESUMEN FINAL
    successful_saves = len([img for img in saved_images if img.get('status') == 'saved'])
    log_success(f"‚úÖ Guardado completado: {successful_saves}/{len(generated_images)} im√°genes generadas + 1 original")
    log_info(f"üìÅ Directorio: {output_dir}")
    
    return original_info, saved_images

# ==================== RUTAS DEL API ====================

@app.route('/health', methods=['GET'])
def health_check():
    """Verificaci√≥n de estado del servicio"""
    try:
        # Verificar conexi√≥n con ComfyUI
        response = requests.get(f"{COMFYUI_URL}/system_stats", timeout=5)
        comfyui_status = "ok" if response.status_code == 200 else "error"
    except:
        comfyui_status = "error"
    
    return jsonify({
        "status": "ok",
        "comfyui_connection": comfyui_status,
        "version": "2.0.0",
        "timestamp": datetime.now().isoformat()
    })

@app.route('/styles', methods=['GET'])
def list_styles():
    """Lista estilos predefinidos disponibles"""
    try:
        styles = get_available_styles()
        return jsonify({
            "styles": styles,
            "total": len(styles),
            "message": "Estilos cargados correctamente"
        })
    except Exception as e:
        log_error(f"Error cargando estilos: {str(e)}")
        return jsonify({"error": str(e)}), 500

@app.route('/workflows', methods=['GET'])
def list_workflows():
    """Lista workflows disponibles organizados por tipo y orientaci√≥n"""
    workflows_structure = {}
    workflows_list = []
    
    if os.path.exists(WORKFLOWS_DIR):
        for root, dirs, files in os.walk(WORKFLOWS_DIR):
            for filename in files:
                if filename.endswith('.json'):
                    # Obtener la ruta relativa desde workflows/
                    rel_path = os.path.relpath(os.path.join(root, filename), WORKFLOWS_DIR)
                    path_parts = rel_path.replace('\\', '/').split('/')
                    
                    # Ejemplo: bathroom/H80x60/cuadro-bathroom-open-H60x802.json
                    if len(path_parts) >= 3:
                        room_type = path_parts[0]  # bathroom
                        orientation = path_parts[1]  # H80x60
                        workflow_name = path_parts[2].replace('.json', '')  # cuadro-bathroom-open-H60x802
                        
                        # ID √∫nico para el workflow
                        workflow_id = f"{room_type}/{orientation}/{workflow_name}"
                        
                        # Crear estructura jer√°rquica
                        if room_type not in workflows_structure:
                            workflows_structure[room_type] = {}
                        if orientation not in workflows_structure[room_type]:
                            workflows_structure[room_type][orientation] = []
                        
                        workflow_info = {
                            "id": workflow_id,
                            "name": workflow_name,
                            "filename": filename,
                            "room_type": room_type,
                            "orientation": orientation,
                            "path": rel_path.replace('\\', '/'),
                            "status": "available"
                        }
                        
                        workflows_structure[room_type][orientation].append(workflow_info)
                        workflows_list.append(workflow_info)
    
    return jsonify({
        "workflows": workflows_list,
        "structure": workflows_structure,
        "total": len(workflows_list),
        "config": WORKFLOW_CONFIG,
        "available_colors": WORKFLOW_CONFIG.get("frame_colors", ["black", "white", "brown", "gold", "silver"]),
        "available_styles": get_available_styles()
    })

@app.route('/process-image', methods=['POST'])
def process_image():
    """Procesa una imagen usando un workflow especificado"""
    job_id = None
    try:
        log_info("=== INICIANDO PROCESAMIENTO DE IMAGEN ===")
        
        # Validar entrada
        if 'image' not in request.files:
            return jsonify({"error": "No se proporcion√≥ imagen"}), 400
        
        file = request.files['image']
        if not file or file.filename == '':
            return jsonify({"error": "Archivo de imagen vac√≠o"}), 400
        
        if not allowed_file(file.filename):
            return jsonify({"error": "Tipo de archivo no permitido"}), 400
        
        # Par√°metros
        workflow_name = request.form.get('workflow', 'default')
        frame_color = request.form.get('frame_color', 'black')
        style_id = request.form.get('style', 'default')
        style_node_id = request.form.get('style_node', None)  # Nodo personalizado para aplicar estilo
        original_filename = request.form.get('original_filename', file.filename)
        include_upscale = request.form.get('include_upscale', 'true').lower() == 'true'  # Incluir imagen upscale en resultados
        
        # ===== CREAR TRABAJO DE SESI√ìN =====
        job_id = session_manager.create_job(
            job_type='individual',
            workflow=workflow_name,
            frame_color=frame_color,
            style=style_id,
            style_node=style_node_id,
            original_filename=original_filename,
            include_upscale=include_upscale  # Agregar par√°metro de upscale
        )
        
        log_info(f"Trabajo de sesi√≥n creado: {job_id}")
        log_info(f"Par√°metros: workflow={workflow_name}, frame_color={frame_color}, style={style_id}, style_node={style_node_id}, file={original_filename}, include_upscale={include_upscale}")
        
        # Actualizar estado del trabajo
        session_manager.update_job(job_id, 
            status='processing', 
            current_operation='Validando par√°metros...'
        )
        
        # Validar color del marco
        if frame_color not in WORKFLOW_CONFIG['frame_colors']:
            frame_color = 'black'
            log_warning(f"Color de marco no v√°lido, usando: {frame_color}")
        
        # Validar estilo
        available_styles = [style['id'] for style in get_available_styles()]
        if style_id not in available_styles:
            style_id = 'default'
            log_warning(f"Estilo no v√°lido, usando: {style_id}")
        
        # Preparar nombres y directorios
        base_name = secure_filename(original_filename.rsplit('.', 1)[0] if '.' in original_filename else 'image')
        output_dir = create_output_directory(base_name)
        
        session_manager.update_job(job_id, current_operation='Preparando archivos...')
        
        # üö´ NO guardar imagen original aqu√≠ - se guardar√° despu√©s con las dem√°s de manera organizada
        log_info("üíæ Imagen original se guardar√° despu√©s junto con las generadas en nuestro directorio personalizado")
        
        # Guardar en input de ComfyUI (para que ComfyUI pueda procesarla)
        input_path, workflow_filename = save_uploaded_image(file, base_name)
        
        session_manager.update_job(job_id, current_operation='Verificando acceso a la imagen...')
        
        # Verificar que ComfyUI puede acceder al archivo (sin fallar si no puede)
        log_info("Verificando acceso a la imagen desde ComfyUI...")
        accessibility_check = verify_image_accessibility(workflow_filename)
        if not accessibility_check:
            log_warning("ComfyUI no puede verificar la imagen, pero continuando...")
            # Esperar un poco m√°s para que el archivo se asiente
            time.sleep(1)
        
        session_manager.update_job(job_id, current_operation='Cargando workflow...')
        
        # Cargar y actualizar workflow
        log_info(f"Cargando workflow: {workflow_name}")
        workflow = load_workflow(workflow_name)
        updated_workflow = update_workflow(workflow, workflow_filename, frame_color, style_id, style_node_id, base_name)
        
        session_manager.update_job(job_id, current_operation='Enviando a ComfyUI...')
        
        # Enviar a ComfyUI
        log_info("Enviando a ComfyUI...")
        prompt_id = submit_workflow_to_comfyui(updated_workflow)
        
        session_manager.update_job(job_id, 
            current_operation='Esperando resultados de ComfyUI...',
            prompt_id=prompt_id
        )
        
        # Esperar resultados
        log_info("Esperando resultados...")
        outputs = wait_for_completion(prompt_id)
        
        session_manager.update_job(job_id, current_operation='Extrayendo im√°genes generadas...')
        
        # Extraer im√°genes generadas
        log_info("Extrayendo im√°genes...")
        generated_images = extract_generated_images(outputs, original_filename, include_upscale)
        
        log_info(f"üìä Resumen de im√°genes extra√≠das: {len(generated_images)} im√°genes (include_upscale={include_upscale})")
        for i, img in enumerate(generated_images):
            log_info(f"   {i+1}. Tipo: {img.get('image_type', 'unknown')}, Archivo: {img['filename']}, Nodo: {img.get('node_id', 'N/A')}")
        
        session_manager.update_job(job_id, current_operation='Guardando im√°genes en nuestro directorio personalizado...')
        
        # üéØ USAR NUESTRO NUEVO SISTEMA DE GUARDADO ORGANIZADO
        log_info("üíæ Guardando im√°genes de manera organizada en nuestro directorio personalizado...")
        original_info, saved_images = save_images_to_our_output(
            output_dir=output_dir,
            original_file=file,
            generated_images=generated_images,
            original_filename=original_filename,
            include_upscale=include_upscale,
            job_id=job_id,
            workflow_name=workflow_name,
            style_id=style_id
        )
        
        # Agregar informaci√≥n adicional a las im√°genes guardadas
        for img in saved_images:
            img['workflow'] = workflow_name
            img['style'] = style_id if style_id != 'default' else 'default'
        
        # üéØ FILTRAR IM√ÅGENES PARA EL FRONTEND - SOLO COMPOSICI√ìN FINAL
        log_info("üéØ Filtrando im√°genes para el frontend - solo composici√≥n final...")
        frontend_images = [img for img in saved_images if img.get('image_type') == 'composition']
        
        if not frontend_images:
            # Fallback: si no hay composici√≥n, buscar cualquier imagen generada
            log_warning("‚ö†Ô∏è No se encontr√≥ imagen de composici√≥n, usando fallback...")
            frontend_images = saved_images[:1]  # Solo la primera
        
        log_info(f"üì§ Para frontend: {len(frontend_images)} imagen(es) de {len(saved_images)} guardadas")
        log_info(f"üíæ Guardadas en disco: {len(saved_images)} im√°genes + 1 original")
        
        # Log detallado de lo que se est√° enviando al frontend vs lo que se guarda
        log_info("üìã RESUMEN DE FILTRADO PARA FRONTEND:")
        log_info(f"   üíæ GUARDADAS EN DISCO ({len(saved_images)} im√°genes):")
        for i, img in enumerate(saved_images):
            tipo = img.get('image_type', 'unknown')
            log_info(f"      {i+1}. {tipo.upper()}: {img['filename']}")
        
        log_info(f"   üì§ ENVIADAS AL FRONTEND ({len(frontend_images)} im√°genes):")
        for i, img in enumerate(frontend_images):
            tipo = img.get('image_type', 'unknown')
            log_info(f"      {i+1}. {tipo.upper()}: {img['filename']}")
        
        # Verificar si se est√°n filtrando upscales
        upscale_count = len([img for img in saved_images if img.get('image_type') == 'upscale'])
        if upscale_count > 0:
            log_success(f"‚úÖ FILTRADO CORRECTO: {upscale_count} imagen(es) upscale guardadas en disco pero excluidas del frontend")
        
        # Crear respuesta
        processing_mode = "text2img + controlnet_0.85" if (style_id and style_id != 'default') else "img2img_preserving_original"
        
        # Seleccionar imagen final principal (composici√≥n)
        final_image_info = None
        final_image_url = None
        if frontend_images:
            final_image_info = frontend_images[0]
            final_image_url = final_image_info['session_url']
            log_success(f"‚úÖ Imagen final para frontend: {final_image_info['filename']} (nodo: {final_image_info.get('node_id', 'N/A')})")
        
        response = {
            "success": True,
            "job_id": job_id,  # Agregar ID del trabajo
            "prompt_id": prompt_id,
            "workflow_used": workflow_name,
            "processing_mode": processing_mode,
            "frame_color": frame_color,
            "style_used": style_id,
            "style_node_used": style_node_id if style_node_id else "auto-detectado",
            "include_upscale": include_upscale,  # Incluir configuraci√≥n de upscale
            "base_name": base_name,
            "output_dir": output_dir,
            # ÔøΩ IMAGEN ORIGINAL: Incluir informaci√≥n de la imagen original guardada
            "original_image": original_info,
            "final_image_url": final_image_url,  # Nueva imagen final para el frontend
            "final_image": final_image_info,  # Informaci√≥n completa de la imagen final
            "generated_images": frontend_images,  # ‚úÖ SOLO IM√ÅGENES PARA FRONTEND (composici√≥n √∫nicamente)
            "total_images": len(frontend_images),  # Total para frontend
            "total_images_saved": len(saved_images),  # Total guardadas en disco
            "total_files_saved": len(saved_images) + 1,  # +1 por la original
            "include_upscale": include_upscale,  # Indicar si se incluy√≥ upscale
            "our_output_dir": OUR_OUTPUT_DIR,  # Informaci√≥n del directorio personalizado
            "message": f"‚úÖ Procesamiento completado en modo {processing_mode}. {len(saved_images) + 1} archivos guardados en directorio personalizado: 1 original + {len(saved_images)} generadas {'(composici√≥n + upscale)' if include_upscale else '(solo composici√≥n)'}. Frontend mostrar√° solo composici√≥n."
        }
        
        # Actualizar trabajo de sesi√≥n como completado
        session_manager.update_job(job_id,
            status='completed',
            current_operation='Completado',
            results=frontend_images,  # ‚úÖ SOLO IM√ÅGENES PARA FRONTEND (composici√≥n √∫nicamente)
            response_data=response
        )
        
        log_success("=== PROCESAMIENTO COMPLETADO ===")
        return jsonify(response)
        
    except FileNotFoundError as e:
        if job_id:
            session_manager.update_job(job_id, status='error', error=f"Workflow no encontrado: {str(e)}")
        log_error(f"Workflow no encontrado: {str(e)}")
        return jsonify({"error": f"Workflow no encontrado: {str(e)}"}), 404
    except TimeoutError as e:
        if job_id:
            session_manager.update_job(job_id, status='error', error=f"Timeout en procesamiento: {str(e)}")
        log_error(f"Timeout en procesamiento: {str(e)}")
        return jsonify({"error": f"Timeout en procesamiento: {str(e)}"}), 408
    except Exception as e:
        if job_id:
            session_manager.update_job(job_id, status='error', error=str(e))
        log_error(f"Error en procesamiento: {str(e)}")
        return jsonify({"error": str(e)}), 500

@app.route('/get-image/<base_name>/<filename>', methods=['GET'])
def get_image(base_name, filename):
    """Descarga una imagen del directorio de salida (mantener compatibilidad)"""
    try:
        # Sanitizar par√°metros
        base_name = secure_filename(base_name)
        filename = secure_filename(filename)
        
        # 1. Buscar primero en nuestro directorio personalizado
        our_file_path = os.path.join(OUR_OUTPUT_DIR, base_name, filename)
        if os.path.exists(our_file_path):
            log_success(f"üì§ Enviando archivo desde nuestro directorio: {our_file_path}")
            return send_file(our_file_path, as_attachment=True, download_name=filename)
        
        # 2. Buscar en el directorio de ComfyUI como respaldo
        comfyui_file_path = os.path.join(COMFYUI_OUTPUT_DIR, base_name, filename)
        if os.path.exists(comfyui_file_path):
            log_success(f"üì§ Enviando archivo desde ComfyUI output: {comfyui_file_path}")
            return send_file(comfyui_file_path, as_attachment=True, download_name=filename)
        
        log_error(f"‚ùå Archivo no encontrado en ning√∫n directorio: {filename}")
        return jsonify({"error": "Archivo no encontrado"}), 404
        
    except Exception as e:
        log_error(f"‚ùå Error al obtener imagen: {str(e)}")
        return jsonify({"error": str(e)}), 500

@app.route('/workflow-nodes/<path:workflow_name>', methods=['GET'])
def get_workflow_nodes(workflow_name):
    """Obtiene los nodos candidatos para aplicar estilos en un workflow espec√≠fico"""
    try:
        log_info(f"üîç Buscando nodos para workflow: {workflow_name}")
        
        # Use the existing load_workflow function that handles path resolution
        workflow_data = load_workflow(workflow_name)
        
        # Collect SeargeTextInputV2 nodes as style candidates
        candidates = []
        for node_id, node in workflow_data.items():
            if node.get('class_type') == 'SeargeTextInputV2':
                title = node.get('_meta', {}).get('title', '')
                current_prompt = node.get('inputs', {}).get('prompt', '')
                
                # Determine if this is a recommended style node
                is_recommended = 'style' in title.lower() or node_id == '6'
                
                # Create node info with all properties the client expects
                node_info = {
                    'id': node_id,
                    'title': title,
                    'name': title or f'Nodo {node_id}',
                    'type': 'SeargeTextInputV2',
                    'description': f'Nodo de texto para prompts ({title})' if title else f'Nodo de texto {node_id}',
                    'recommended': is_recommended,
                    'current_value': current_prompt[:100] + '...' if len(current_prompt) > 100 else current_prompt,
                    'class_type': node.get('class_type')
                }
                
                candidates.append(node_info)
                log_info(f"üìù Nodo encontrado: {node_id} - {title} (recomendado: {is_recommended})")
        
        # Sort candidates: recommended first, then by node ID
        candidates.sort(key=lambda x: (not x['recommended'], int(x['id']) if x['id'].isdigit() else x['id']))
        
        log_success(f"‚úÖ {len(candidates)} nodos candidatos encontrados para {workflow_name}")
        return jsonify({
            'success': True,
            'workflow_name': workflow_name, 
            'nodes': candidates,
            'candidate_nodes': candidates,  # For client compatibility
            'total_nodes': len(candidates)
        })
        
    except FileNotFoundError:
        log_error(f"‚ùå Workflow no encontrado: {workflow_name}")
        return jsonify({'success': False, 'error': 'Workflow not found'}), 404
    except Exception as e:
        log_error(f"‚ùå Error getting workflow nodes for {workflow_name}: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/batch-status/<batch_id>', methods=['GET'])
def get_batch_status(batch_id):
    """
    Obtiene el status actual de un batch en progreso con informaci√≥n de sesi√≥n
    Consulta tanto ACTIVE_BATCHES como el sistema de sesi√≥n para persistencia
    """
    batch_info = None
    
    # 1. Primero intentar obtener de ACTIVE_BATCHES (batch en progreso)
    with BATCH_LOCK:
        if batch_id in ACTIVE_BATCHES:
            batch_info = ACTIVE_BATCHES[batch_id].copy()
    
    # 2. Si no est√° en ACTIVE_BATCHES, buscar en el sistema de sesi√≥n
    if not batch_info:
        # Buscar en todos los jobs de sesi√≥n por batch_tracking_id
        all_jobs = session_manager.get_all_active_jobs()
        for job_id, job_data in all_jobs.items():
            if job_data.get('batch_tracking_id') == batch_id:
                # Reconstruir batch_info desde el job de sesi√≥n
                images = session_manager.get_job_images(job_id)
                batch_info = {
                    'batch_id': batch_id,
                    'session_job_id': job_id,
                    'status': job_data.get('status', 'completed'),
                    'total_workflows': job_data.get('total_workflows', len(images)),
                    'completed_workflows': len(images),
                    'errors': job_data.get('errors', []),
                    'all_images': images,
                    'created_at': job_data.get('created_at'),
                    'type': 'batch'
                }
                break
    
    if not batch_info:
        return jsonify({"error": "Batch no encontrado"}), 404
    
    # 3. Agregar informaci√≥n del job de sesi√≥n si est√° disponible
    session_job_id = batch_info.get('session_job_id')
    if session_job_id:
        session_job = session_manager.get_job(session_job_id)
        if session_job:
            batch_info['session_job'] = {
                'id': session_job_id,
                'status': session_job.get('status'),
                'created_at': session_job.get('created_at'),
                'original_image_url': f"/session/images/{session_job_id}/original.png",
                'results_count': len(session_job.get('results', []))
            }
    
    return jsonify(batch_info)

@app.route('/batch-status/<batch_id>', methods=['DELETE'])
def clear_batch_status(batch_id):
    """
    Limpia un batch completado del tracking (mantiene la sesi√≥n persistente)
    """
    with BATCH_LOCK:
        if batch_id in ACTIVE_BATCHES:
            session_job_id = ACTIVE_BATCHES[batch_id].get('session_job_id')
            del ACTIVE_BATCHES[batch_id]
            
            message = f"Batch {batch_id} eliminado del tracking"
            if session_job_id:
                message += f". Job de sesi√≥n {session_job_id} mantenido para persistencia"
            
            return jsonify({
                "success": True, 
                "message": message,
                "session_job_id": session_job_id
            })
        else:
            return jsonify({"error": "Batch no encontrado"}), 404

@app.route('/active-batches', methods=['GET'])
def get_active_batches():
    """
    Obtiene la lista de todos los batches activos
    """
    with BATCH_LOCK:
        batches = {bid: {"status": info["status"], "total_workflows": info["total_workflows"], 
                        "completed_workflows": info["completed_workflows"]} 
                  for bid, info in ACTIVE_BATCHES.items()}
    
    return jsonify({"active_batches": batches, "count": len(batches)})

# ===== ENDPOINTS DE PERSISTENCIA DE SESI√ìN =====

@app.route('/session/jobs', methods=['GET'])
def get_session_jobs():
    """Obtiene todos los trabajos de la sesi√≥n actual"""
    try:
        jobs = session_manager.get_all_active_jobs()
        summary = session_manager.get_session_summary()
        
        return jsonify({
            "success": True,
            "jobs": jobs,
            "summary": summary
        })
    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500

@app.route('/session/jobs/<job_id>', methods=['GET'])
def get_session_job(job_id):
    """Obtiene un trabajo espec√≠fico de la sesi√≥n"""
    try:
        job = session_manager.get_job(job_id)
        if job:
            # Agregar URLs de im√°genes si existen
            job['image_urls'] = session_manager.get_job_images(job_id)
            return jsonify({"success": True, "job": job})
        else:
            return jsonify({"success": False, "error": "Trabajo no encontrado"}), 404
    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500

@app.route('/session/jobs/<job_id>', methods=['DELETE'])
def delete_session_job(job_id):
    """Elimina un trabajo de la sesi√≥n"""
    try:
        success = session_manager.delete_job(job_id)
        if success:
            return jsonify({"success": True, "message": "Trabajo eliminado"})
        else:
            return jsonify({"success": False, "error": "Trabajo no encontrado"}), 404
    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500

@app.route('/session/images/<job_id>/<filename>', methods=['GET'])
def serve_session_image(job_id, filename):
    """Sirve im√°genes de trabajos de sesi√≥n"""
    try:
        # Sanitizar par√°metros
        job_id = secure_filename(job_id)
        filename = secure_filename(filename)
        
        image_path = os.path.join(session_manager.session_dir, job_id, filename)
        
        log_info(f"Buscando imagen de sesi√≥n: {image_path}")
        
        if os.path.exists(image_path):
            log_success(f"Imagen de sesi√≥n encontrada: {image_path}")
            return send_file(image_path)
        else:
            log_error(f"Imagen de sesi√≥n no encontrada: {image_path}")
            return jsonify({"error": "Imagen no encontrada"}), 404
    except Exception as e:
        log_error(f"Error sirviendo imagen de sesi√≥n: {str(e)}")
        return jsonify({"error": str(e)}), 500

@app.route('/session/cleanup', methods=['POST'])
def cleanup_session():
    """Limpia trabajos antiguos de la sesi√≥n"""
    try:
        hours = request.json.get('hours', 24) if request.is_json else 24
        cleaned_count = session_manager.cleanup_old_jobs(hours)
        
        return jsonify({
            "success": True,
            "message": f"Se limpiaron {cleaned_count} trabajos antiguos",
            "cleaned_count": cleaned_count
        })
    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500

@app.route('/session/clear', methods=['POST'])
def clear_session():
    """Limpia completamente toda la sesi√≥n - todos los trabajos e im√°genes"""
    try:
        log_info("üßπ Iniciando limpieza completa de sesi√≥n...")
        
        result = session_manager.clear_all_session()
        
        if result["success"]:
            log_success(f"‚úÖ {result['message']}")
        else:
            log_error(f"‚ùå Error limpiando sesi√≥n: {result['error']}")
            
        return jsonify(result)
        
    except Exception as e:
        log_error(f"‚ùå Error en endpoint clear_session: {str(e)}")
        return jsonify({
            "success": False, 
            "error": str(e),
            "jobs_cleared": 0,
            "session_reset": False
        }), 500

@app.route('/')
def serve_client():
    """Sirve el cliente web principal"""
    client_path = os.path.join(BASE_DIR, 'web_client_fixed.html')
    if os.path.exists(client_path):
        return send_file(client_path)
    else:
        # Fallback al cliente original
        fallback_path = os.path.join(BASE_DIR, 'web_client.html')
        if os.path.exists(fallback_path):
            return send_file(fallback_path)
        else:
            return jsonify({
                "message": "ComfyUI API REST - Nueva implementaci√≥n",
                "version": "2.0.0",
                "endpoints": ["/health", "/workflows", "/styles", "/workflow-nodes/<workflow_name>", "/process-image", "/process-batch", "/get-image"],
                "web_clients": ["/web_client_fixed.html", "/web_client.html"]
            })

@app.route('/web_client_fixed.html')
def serve_fixed_client():
    """Sirve el cliente web corregido"""
    client_path = os.path.join(BASE_DIR, 'web_client_fixed.html')
    if os.path.exists(client_path):
        return send_file(client_path)
    else:
        return jsonify({"error": "Cliente web corregido no encontrado"}), 404

@app.route('/web_client.html')
def serve_original_client():
    """Servir el cliente web original (fallback)"""
    try:
        return send_file('web_client.html')
    except FileNotFoundError:
        return jsonify({
            "error": "Cliente web original no encontrado",
            "message": "Use /web_client_fixed.html en su lugar"
        }), 404

# ==================== FUNCIONES AUXILIARES ADICIONALES ====================

def is_allowed_file(filename):
    """Verifica si el archivo tiene una extensi√≥n permitida (alias de allowed_file)"""
    return allowed_file(filename)

def get_available_workflows():
    """Obtiene la lista de workflows disponibles"""
    workflows = []
    
    if os.path.exists(WORKFLOWS_DIR):
        for root, dirs, files in os.walk(WORKFLOWS_DIR):
            for filename in files:
                if filename.endswith('.json'):
                    # Obtener la ruta relativa desde workflows/
                    rel_path = os.path.relpath(os.path.join(root, filename), WORKFLOWS_DIR)
                    path_parts = rel_path.replace('\\', '/').split('/')
                    
                    # Ejemplo: bathroom/H80x60/cuadro-bathroom-open-H60x802.json
                    if len(path_parts) >= 3:
                        room_type = path_parts[0]  # bathroom
                        orientation = path_parts[1]  # H80x60
                        workflow_name = path_parts[2].replace('.json', '')  # cuadro-bathroom-open-H60x802
                        
                        # ID √∫nico para el workflow
                        workflow_id = f"{room_type}/{orientation}/{workflow_name}"
                        
                        workflow_info = {
                            "id": workflow_id,
                            "name": workflow_name,
                            "filename": filename,
                            "room_type": room_type,
                            "orientation": orientation,
                            "path": rel_path.replace('\\', '/'),
                            "status": "available"
                        }
                        
                        workflows.append(workflow_info)
    
    return workflows

# ==================== PROCESAMIENTO EN LOTE ====================

@app.route('/process-batch', methods=['POST'])
def process_batch():
    """
    Procesa una imagen con m√∫ltiples workflows simult√°neamente con persistencia
    """
    batch_job_id = None
    try:
        log_info("üì¶ Iniciando procesamiento en lote...")
        
        # Validar datos de entrada
        if 'image' not in request.files or not request.files['image']:
            return jsonify({"error": "No se encontr√≥ archivo de imagen"}), 400
            
        image_file = request.files['image']
        
        if not is_allowed_file(image_file.filename):
            return jsonify({"error": f"Formato de archivo no permitido. Usar: {', '.join(WORKFLOW_CONFIG['allowed_extensions'])}"}), 400
        
        # Obtener par√°metros del batch
        batch_config = {}
        
        # Verificar si se envi√≥ como JSON en batch_config
        if 'batch_config' in request.form:
            try:
                batch_config = json.loads(request.form['batch_config'])
                log_info(f"üìã Configuraci√≥n del batch desde JSON: {batch_config}")
            except json.JSONDecodeError:
                log_warning("‚ö†Ô∏è Error decodificando batch_config JSON, usando par√°metros individuales")
        
        # Filtros opcionales (fallback a par√°metros individuales)
        if not batch_config.get('room_types') and 'room_types' in request.form:
            room_types_str = request.form['room_types']
            batch_config['room_types'] = [t.strip() for t in room_types_str.split(',') if t.strip()]
            
        if not batch_config.get('orientations') and 'orientations' in request.form:
            orientations_str = request.form['orientations']
            batch_config['orientations'] = [o.strip() for o in orientations_str.split(',') if o.strip()]
            
        if not batch_config.get('specific_workflows') and 'specific_workflows' in request.form:
            specific_str = request.form['specific_workflows']
            batch_config['specific_workflows'] = [w.strip() for w in specific_str.split(',') if w.strip()]
        
        # Par√°metros comunes (pueden venir del JSON o del form)
        frame_color = batch_config.get('frame_color') or request.form.get('frame_color', 'black')
        style = batch_config.get('style') or request.form.get('style', 'default')
        include_upscale = batch_config.get('include_upscale', True)  # Default True para compatibilidad
        if isinstance(include_upscale, str):
            include_upscale = include_upscale.lower() == 'true'
        original_filename = request.form.get('original_filename', image_file.filename)
        
        # Verificar que tenemos un nombre v√°lido
        if not original_filename or original_filename.strip() == '':
            original_filename = image_file.filename
            log_warning(f"‚ö†Ô∏è original_filename vac√≠o, usando filename del archivo: '{original_filename}'")
        
        log_info(f"üìã Nombre archivo original para batch: '{original_filename}' (de form: '{request.form.get('original_filename')}', filename: '{image_file.filename}')")
        log_info(f"üìà Incluir upscale en batch: {include_upscale}")
        
        # ===== CREAR TRABAJO DE SESI√ìN PARA BATCH =====
        batch_job_id = session_manager.create_job(
            job_type='batch',
            batch_config=batch_config,
            frame_color=frame_color,
            style=style,
            include_upscale=include_upscale,
            original_filename=original_filename,
            total_workflows=0,  # Se actualizar√° despu√©s
            completed_workflows=0,
            successful=0,
            failed=0
        )
        
        log_info(f"üÜî Trabajo de sesi√≥n batch creado: {batch_job_id}")
        
        # Actualizar estado del trabajo
        session_manager.update_job(batch_job_id, 
            status='processing', 
            current_operation='Validando par√°metros del batch...'
        )
        
        log_info(f"üìã Configuraci√≥n del batch: {batch_config}")
        log_info(f"üé® Par√°metros: frame_color={frame_color}, style={style}")
        
        # Actualizar estado del trabajo
        session_manager.update_job(batch_job_id, current_operation='Validando estilo...')
        
        # Validar estilo
        available_styles = get_available_styles()
        available_style_ids = [style['id'] for style in available_styles]
        if style not in available_style_ids:
            session_manager.update_job(batch_job_id, status='error', error=f"Estilo no encontrado: {style}")
            return jsonify({"error": f"Estilo no encontrado: {style}. Estilos disponibles: {available_style_ids}"}), 400
        
        # Actualizar estado del trabajo
        session_manager.update_job(batch_job_id, current_operation='Obteniendo workflows disponibles...')
        
        # Obtener workflows disponibles
        available_workflows = get_available_workflows()
        
        # Actualizar estado del trabajo
        session_manager.update_job(batch_job_id, current_operation='Filtrando workflows...')
        
        # Filtrar workflows seg√∫n criterios del batch
        filtered_workflows = filter_workflows_for_batch(batch_config, available_workflows)
        
        if not filtered_workflows:
            error_msg = "No se encontraron workflows que coincidan con los criterios especificados"
            session_manager.update_job(batch_job_id, status='error', error=error_msg)
            return jsonify({
                "error": error_msg,
                "available_workflows": len(available_workflows),
                "criteria": batch_config
            }), 400
        
        # Actualizar total de workflows en el job de sesi√≥n
        session_manager.update_job(batch_job_id, 
            total_workflows=len(filtered_workflows),
            workflows=[w["id"] for w in filtered_workflows],
            current_operation=f'Preparando procesamiento de {len(filtered_workflows)} workflows...'
        )
        
        log_info(f"üéØ Workflows seleccionados: {len(filtered_workflows)}/{len(available_workflows)}")
        
        # Guardar imagen original en la sesi√≥n inmediatamente

        session_manager.update_job(batch_job_id, current_operation='Guardando imagen original...')
        image_file.seek(0)
        original_image_data = image_file.read()
        session_original_url = session_manager.save_job_image(batch_job_id, original_image_data, 'original.png')
        log_info(f"üñºÔ∏è Imagen original guardada en sesi√≥n: {session_original_url}")
        
        # Preparar par√°metros comunes
        common_params = {
            "frame_color": frame_color,
            "style": style,
            "include_upscale": include_upscale,
            "original_filename": original_filename  # Agregar nombre de imagen original
        }
        
        log_info(f"üîß Par√°metros comunes para batch: {common_params}")
        
        # Obtener nodos de estilo si es necesario
        if filtered_workflows:
            sample_workflow = load_workflow(filtered_workflows[0]["id"])
            style_nodes = get_workflow_nodes_for_style(sample_workflow)
            if style_nodes:
                common_params["style_node"] = style_nodes[0]["id"]
        
        # Generar ID √∫nico para este batch (diferente del job_id de sesi√≥n)
        batch_id = datetime.now().strftime("%Y%m%d_%H%M%S") + "_" + str(uuid.uuid4())[:8]
        
        # Almacenar referencia entre batch_id y job_id de sesi√≥n
        session_manager.update_job(batch_job_id, batch_tracking_id=batch_id)
        
        # Inicializar tracking del batch
        with BATCH_LOCK:
            ACTIVE_BATCHES[batch_id] = {
                "batch_id": batch_id,
                "session_job_id": batch_job_id,  # Referencia al job de sesi√≥n
                "status": "starting",
                "total_workflows": len(filtered_workflows),
                "completed_workflows": 0,
                "successful": 0,
                "failed": 0,
                "results": [],
                "start_time": time.time(),
                "estimated_completion": None,
                "current_operation": "Iniciando procesamiento...",
                "workflow_list": [w["id"] for w in filtered_workflows]
            }
        
        log_info(f"üöÄ Iniciando procesamiento simult√°neo de {len(filtered_workflows)} workflows...")
        log_info(f"üìä Batch ID para tracking: {batch_id}")
        log_info(f"üÜî Session Job ID: {batch_job_id}")
        
        # üéØ APLICAR THROTTLING DE BATCH
        log_info(f"‚è∞ Aplicando throttling de batch para {len(filtered_workflows)} workflows...")
        throttle_wait_time = enforce_batch_throttle(len(filtered_workflows))
        
        if throttle_wait_time > 0:
            session_manager.update_job(batch_job_id, 
                current_operation=f'Throttling aplicado: esper√≥ {throttle_wait_time:.1f}s para evitar sobrecarga de ComfyUI'
            )
        
        # Guardar el contenido de la imagen en memoria antes de iniciar el thread
        image_file.seek(0)
        image_data = BytesIO(image_file.read())
        image_data.seek(0)
        
        # Iniciar procesamiento en thread separado
        def process_batch_async():
            try:
                results = process_all_workflows_simult√°neamente_with_tracking(
                    image_data, filtered_workflows, common_params, batch_id, batch_job_id
                )
                
                # Finalizar batch Y sesi√≥n
                with BATCH_LOCK:
                    if batch_id in ACTIVE_BATCHES:
                        batch_info = ACTIVE_BATCHES[batch_id]
                        batch_info["status"] = "completed"
                        batch_info["total_processing_time"] = round(time.time() - batch_info["start_time"], 2)
                        batch_info["final_results"] = results
                
                # Finalizar job de sesi√≥n con URLs de sesi√≥n mejoradas
                successful_results = [r for r in results if r.get('success', False)]
                failed_results = [r for r in results if not r.get('success', False)]
                
                # üî• RECOLECTAR TODAS LAS URLs DE SESI√ìN DE LOS RESULTADOS
                all_session_images = []
                for result in successful_results:
                    if result.get('session_images'):
                        all_session_images.extend(result['session_images'])
                    # Tambi√©n buscar en generated_images por session_url
                    if result.get('generated_images'):
                        for img in result['generated_images']:
                            if img.get('session_url') and img['session_url'] not in all_session_images:
                                all_session_images.append(img['session_url'])
                
                log_info(f"üìä Finalizando batch - Total URLs de sesi√≥n recolectadas: {len(all_session_images)}")
                
                # Preparar results mejorados para sesi√≥n
                session_results = []
                for result in results:
                    if result.get('success') and result.get('generated_images'):
                        for img in result['generated_images']:
                            session_result = {
                                'filename': img.get('filename'),
                                'url': img.get('url'),
                                'session_url': img.get('session_url'),
                                'workflow': result.get('workflow_id'),
                                'image_type': img.get('image_type', 'composition'),
                                'status': 'completed'
                            }
                            session_results.append(session_result)
                
                session_manager.update_job(batch_job_id,
                    status='completed',
                    successful=len(successful_results),
                    failed=len(failed_results),
                    results=session_results,  # üî• Usar resultados optimizados para sesi√≥n
                    completed_workflows=len(successful_results),
                    current_operation=f'Completado: {len(successful_results)} exitosos, {len(failed_results)} fallidos'
                )
                
                log_success(f"‚úÖ Batch {batch_id} completado: {len(successful_results)}/{len(results)} exitosos")
                        
            except Exception as e:
                log_error(f"‚ùå Error en procesamiento async de batch {batch_id}: {str(e)}")
                with BATCH_LOCK:
                    if batch_id in ACTIVE_BATCHES:
                        ACTIVE_BATCHES[batch_id]["status"] = "error"
                        ACTIVE_BATCHES[batch_id]["error"] = str(e)
                
                # Actualizar job de sesi√≥n con error pero conservando posibles im√°genes ya procesadas
                existing_images = session_manager.get_job_images(batch_job_id)
                session_manager.update_job(batch_job_id, 
                    status='error', 
                    error=str(e),
                    current_operation=f'Error: {str(e)}',
                    # Conservar im√°genes ya procesadas si existen
                    completed_workflows=len(existing_images) if existing_images else 0
                )
        
        # Iniciar thread
        thread = threading.Thread(target=process_batch_async)
        thread.daemon = True
        thread.start()
        
        # Retornar inmediatamente el ID del batch para tracking
        return jsonify({
            "success": True,
            "batch_id": batch_id,
            "session_job_id": batch_job_id,  # Tambi√©n devolver el job ID de sesi√≥n
            "processing_mode": "asynchronous_with_tracking_and_throttling",
            "status": "processing",
            "total_workflows": len(filtered_workflows),
            "throttling_info": {
                "throttle_applied": throttle_wait_time > 0,
                "throttle_wait_time": round(throttle_wait_time, 1),
                "estimated_sending_time": 0,  # Sin delay entre prompts
                "prompt_send_delay": BATCH_PROMPT_SEND_DELAY,
                "immediate_sending": True
            },
            "message": "Batch iniciado con throttling. Use GET /batch-status/{batch_id} para consultar progreso o /session/jobs/{session_job_id} para persistencia",
            "status_endpoint": f"/batch-status/{batch_id}",
            "session_endpoint": f"/session/jobs/{batch_job_id}",
            "workflow_list": [w["id"] for w in filtered_workflows],
            "batch_config": batch_config,
            "common_params": common_params,
            "original_image_url": session_original_url
        })
        
    except Exception as e:
        log_error(f"‚ùå Error en procesamiento batch: {str(e)}")
        
        # Actualizar job de sesi√≥n si existe
        if batch_job_id:
            session_manager.update_job(batch_job_id, status='error', error=str(e))
        
        return jsonify({
            "success": False, 
            "error": str(e),
            "processing_mode": "simultaneous",
            "session_job_id": batch_job_id
        }), 500

@app.route('/batch-throttle-status', methods=['GET'])
def get_batch_throttle_status():
    """
    Devuelve el estado actual del throttling de batches
    """
    try:
        current_time = time.time()
        
        with BATCH_THROTTLE_LOCK:
            # Calcular tiempo restante para poder enviar el siguiente lote
            remaining_wait_time = calculate_batch_throttle_delay(0)  # 0 prompts para solo calcular
            
            # Informaci√≥n sobre el lote actual
            time_since_last_batch = current_time - LAST_BATCH_SUBMIT_TIME if LAST_BATCH_SUBMIT_TIME > 0 else 0
            # Tiempo estimado de procesamiento m√≠nimo (1s por prompt)
            estimated_completion_time = LAST_BATCH_SUBMIT_TIME + (CURRENT_BATCH_PROMPTS * 1.0)
            
            return jsonify({
                "success": True,
                "throttle_status": {
                    "can_send_batch": remaining_wait_time == 0,
                    "remaining_wait_time": round(remaining_wait_time, 1),
                    "current_batch_prompts": CURRENT_BATCH_PROMPTS,
                    "prompt_send_delay": BATCH_PROMPT_SEND_DELAY,
                    "immediate_sending": True,
                    "last_batch_submit_time": LAST_BATCH_SUBMIT_TIME,
                    "time_since_last_batch": round(time_since_last_batch, 1),
                    "estimated_completion_time": estimated_completion_time,
                    "current_time": current_time
                }
            })
    except Exception as e:
        return jsonify({
            "success": False,
            "error": str(e)
        }), 500

def filter_workflows_for_batch(batch_config, available_workflows):
    """
    Filtra workflows seg√∫n los criterios del batch
    """
    filtered = []
    
    room_types = batch_config.get("room_types", [])
    orientations = batch_config.get("orientations", [])
    specific_workflows = batch_config.get("specific_workflows", [])
    
    for workflow in available_workflows:
        # Si hay workflows espec√≠ficos, usar solo esos
        if specific_workflows and workflow["id"] not in specific_workflows:
            continue
            
        # Filtrar por tipo de habitaci√≥n
        if room_types and workflow["room_type"] not in room_types:
            continue
            
        # Filtrar por orientaci√≥n  
        if orientations and workflow["orientation"] not in orientations:
            continue
            
        filtered.append(workflow)
    
    return filtered

def process_single_workflow_for_batch(image_file, workflow_info, common_params):
    """
    Procesa la imagen con un workflow espec√≠fico para el batch
    """
    try:
        # Reutilizar la l√≥gica existente de process_image pero adaptada
        start_time = time.time()
        
        # Guardar imagen temporal
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"batch_{timestamp}_{workflow_info['id'].replace('/', '_')}.{image_file.filename.split('.')[-1]}"
        
        input_path = os.path.join(COMFYUI_INPUT_DIR, filename)
        image_file.seek(0)  # Reset file pointer
        image_file.save(input_path)
        
        # Cargar y actualizar workflow
        workflow = load_workflow(workflow_info["id"])
        workflow = update_workflow(
            workflow, 
            filename, 
            common_params["frame_color"], 
            common_params["style"], 
            common_params.get("style_node"),
            workflow_info["id"]  # output_subfolder
        )
        
        # Enviar a ComfyUI
        prompt_id = submit_workflow_to_comfyui(workflow)
        if not prompt_id:
            raise Exception("Error enviando workflow a ComfyUI")
        
        # Esperar resultados
        outputs = wait_for_completion(prompt_id)
        if not outputs:
            raise Exception("Error esperando resultados de ComfyUI")
        
        # Extraer im√°genes generadas
        original_image_name = common_params.get('original_filename', 'batch_image')
        include_upscale = common_params.get('include_upscale', True)
        generated_images = extract_generated_images(outputs, original_image_name, include_upscale)
        
        # Crear directorio de salida basado en nombre de imagen original
        # Usar la misma l√≥gica que en process_image individual
        original_image_name = common_params.get('original_filename', 'batch_image')
        base_image_name = secure_filename(original_image_name.rsplit('.', 1)[0] if '.' in original_image_name else 'image')
        
        log_info(f"üìÅ Creando directorio para batch: original_filename='{original_image_name}' -> base_name='{base_image_name}'")
        
        # Crear carpeta con nombre de imagen original
        batch_output_dir = create_output_directory(base_image_name)
        
        # Copiar im√°genes generadas con nombre unificado (evitando duplicados)
        # Las im√°genes en generated_images ya est√°n filtradas seg√∫n include_upscale
        saved_images = []
        include_upscale = common_params.get('include_upscale', True)
        log_info(f"üìä Batch - Resumen de im√°genes extra√≠das: {len(generated_images)} im√°genes (include_upscale={include_upscale})")
        for i, img in enumerate(generated_images):
            log_info(f"   {i+1}. Tipo: {img.get('image_type', 'unknown')}, Archivo: {img['filename']}, Nodo: {img.get('node_id', 'N/A')}")
        
        for img_info in generated_images:
            
            source_path = find_image_file(img_info['filename'], img_info['subfolder'])
            if source_path:
                # Crear nombre unificado: workflow_estilo_fecha_numero.extension
                style_name = common_params.get('style', 'default')
                workflow_clean = workflow_info['id'].replace('/', '_').replace('-', '_')
                
                # Obtener extensi√≥n original
                if '.' in img_info['filename']:
                    original_ext = img_info['filename'].rsplit('.', 1)[1]
                else:
                    original_ext = 'png'
                
                # Generar nombre √∫nico: workflow_estilo_fechahora_numero.extension
                img_number = len(saved_images) + 1
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                new_filename = f"{workflow_clean}_{style_name}_{timestamp}_{img_number:03d}.{original_ext}"
                
                dest_path = os.path.join(batch_output_dir, new_filename)
                
                # ‚úÖ VERIFICAR SI YA EXISTE PARA EVITAR DUPLICADOS
                if os.path.exists(dest_path):
                    log_warning(f"‚ö†Ô∏è Archivo de lote ya existe, saltando: {new_filename}")
                    # A√∫n as√≠ lo agregamos a la lista con la info existente
                    saved_images.append({
                        'filename': new_filename,
                        'url': f"/get-image/{base_image_name}/{new_filename}",
                        'original_filename': img_info['filename'],
                        'workflow': workflow_info['id'],
                        'style': style_name,
                        'status': 'existing'  # Marcar como existente
                    })
                    continue
                
                # Copiar archivo si no existe
                shutil.copy2(source_path, dest_path)
                saved_images.append({
                    'filename': new_filename,
                    'url': f"/get-image/{base_image_name}/{new_filename}",
                    'original_filename': img_info['filename'],
                    'workflow': workflow_info['id'],
                    'style': style_name,
                    'status': 'new'  # Marcar como nuevo
                })
                log_success(f"‚úÖ Imagen de lote nueva copiada: {dest_path}")
        
        # Guardar imagen original solo una vez por batch (no por workflow)
        original_dest = os.path.join(batch_output_dir, 'original.png')
        if not os.path.exists(original_dest):  # Solo si no existe ya
            image_file.seek(0)
            image = Image.open(image_file.stream)
            image.save(original_dest, format='PNG')
        
        processing_time = time.time() - start_time
        
        # üéØ FILTRAR IM√ÅGENES PARA EL FRONTEND EN BATCH - SOLO COMPOSICI√ìN FINAL
        log_info(f"üéØ Filtrando im√°genes para frontend en batch - solo composici√≥n final...")
        frontend_images = []
        
        # En batch, todas las im√°genes guardadas en saved_images ya son filtradas por extract_generated_images
        # pero necesitamos filtrar a√∫n m√°s para el frontend (solo composici√≥n)
        for img in saved_images:
            # En batch, las im√°genes no tienen 'image_type' porque son renombradas,
            # pero sabemos que si include_upscale=False, solo hay composici√≥n
            # Si include_upscale=True, la primera imagen es composici√≥n y la segunda upscale
            if len(frontend_images) == 0:  # Siempre incluir la primera (composici√≥n)
                frontend_images.append(img)
                log_info(f"‚úÖ Batch - Imagen de composici√≥n para frontend: {img['filename']}")
            # No incluir m√°s im√°genes (evitar upscale en frontend)
        
        log_info(f"üì§ Batch - Para frontend: {len(frontend_images)} imagen(es) de {len(saved_images)} guardadas")
        log_info(f"üíæ Batch - Guardadas en disco: {len(saved_images)} im√°genes + 1 original")
        
        return {
            "workflow_id": workflow_info["id"],
            "workflow_info": workflow_info,
            "success": True,
            "generated_images": frontend_images,  # ‚úÖ SOLO IM√ÅGENES PARA FRONTEND (composici√≥n √∫nicamente)
            "all_images_saved": saved_images,  # Todas las im√°genes guardadas en disco (para debugging)
            "original_image": {
                "filename": "original.png",
                "url": f"/get-image/{base_image_name}/original.png"
            },
            "processing_time": round(processing_time, 2)
        }
        
    except Exception as e:
        log_error(f"Error procesando workflow {workflow_info['id']}: {str(e)}")
        return {
            "workflow_id": workflow_info["id"],
            "workflow_info": workflow_info,
            "success": False,
            "error": str(e),
            "processing_time": 0
        }

def process_all_workflows_simult√°neamente_with_tracking(image_data, workflows, common_params, batch_id, session_job_id=None):
    """
    Procesa todos los workflows simult√°neamente con tracking en tiempo real y persistencia
    image_data: BytesIO object con el contenido de la imagen
    session_job_id: ID del job de sesi√≥n para persistencia
    """
    import concurrent.futures
    
    results = []
    submitted_prompts = {}  # prompt_id -> workflow_info
    
    # Actualizar status: enviando workflows
    with BATCH_LOCK:
        if batch_id in ACTIVE_BATCHES:
            ACTIVE_BATCHES[batch_id]["status"] = "submitting"
            ACTIVE_BATCHES[batch_id]["current_operation"] = "Enviando workflows a ComfyUI..."
    
    # Actualizar tambi√©n el job de sesi√≥n
    if session_job_id:
        session_manager.update_job(session_job_id, 
            status='processing',
            current_operation="Enviando workflows a ComfyUI..."
        )
    
    log_info(f"üöÄ Enviando {len(workflows)} workflows simult√°neamente a ComfyUI...")
    log_info(f"‚ö° Env√≠o inmediato sin delay entre prompts")
    
    # Pre-cargar la imagen una vez para todos los workflows
    log_info("üì∑ Pre-cargando imagen para todos los workflows...")
    try:
        image_data.seek(0)
        master_image = Image.open(image_data)
        
        # Convertir a RGB si es necesario
        if master_image.mode in ('RGBA', 'LA', 'P'):
            background = Image.new('RGB', master_image.size, (255, 255, 255))
            if master_image.mode == 'P':
                master_image = master_image.convert('RGBA')
            background.paste(master_image, mask=master_image.split()[-1] if master_image.mode in ('RGBA', 'LA') else None)
            master_image = background
        elif master_image.mode != 'RGB':
            master_image = master_image.convert('RGB')
            
        # Redimensionar si es muy grande
        max_size = 2048
        if master_image.width > max_size or master_image.height > max_size:
            master_image.thumbnail((max_size, max_size), Image.Resampling.LANCZOS)
            log_info(f"üìè Imagen redimensionada a: {master_image.width}x{master_image.height}")
            
        log_success("‚úÖ Imagen pre-cargada correctamente")
        
    except Exception as e:
        log_error(f"‚ùå Error pre-cargando imagen: {str(e)}")
        with BATCH_LOCK:
            if batch_id in ACTIVE_BATCHES:
                ACTIVE_BATCHES[batch_id]["status"] = "error"
                ACTIVE_BATCHES[batch_id]["error"] = f"Error pre-cargando imagen: {str(e)}"
        
        # Actualizar tambi√©n el job de sesi√≥n
        if session_job_id:
            session_manager.update_job(session_job_id, 
                status='error',
                error=f"Error pre-cargando imagen: {str(e)}"
            )
        return []

    # Fase 1: Preparar y enviar todos los workflows a ComfyUI sin delay
    start_sending_time = time.time()
    
    for i, workflow_info in enumerate(workflows):
        try:
            # Actualizar progreso
            with BATCH_LOCK:
                if batch_id in ACTIVE_BATCHES:
                    elapsed_sending = time.time() - start_sending_time
                    ACTIVE_BATCHES[batch_id]["current_operation"] = f"Enviando {i+1}/{len(workflows)}: {workflow_info['id']} (‚ö° {elapsed_sending:.1f}s)"
            
            log_info(f"üì§ Preparando {i+1}/{len(workflows)}: {workflow_info['id']}")
            
            # Sin delay entre prompts - env√≠o inmediato
            
            # Guardar imagen temporal √∫nica para cada workflow
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            unique_filename = f"batch_{batch_id}_{i:03d}_{workflow_info['id'].replace('/', '_')}.png"
            
            input_path = os.path.join(COMFYUI_INPUT_DIR, unique_filename)
            
            # Guardar copia de la imagen pre-cargada
            master_image.save(input_path, format='PNG', optimize=False)
            
            # Cargar y actualizar workflow
            # Usar el nombre base de la imagen original para mantener consistencia
            original_image_name = common_params.get('original_filename', 'batch_image')
            base_image_name = secure_filename(original_image_name.rsplit('.', 1)[0] if '.' in original_image_name else 'image')
            
            log_info(f"üîß Workflow {i+1}/{len(workflows)} - original: '{original_image_name}' -> base: '{base_image_name}'")
            
            workflow = load_workflow(workflow_info["id"])
            workflow = update_workflow(
                workflow, 
                unique_filename, 
                common_params["frame_color"], 
                common_params["style"], 
                common_params.get("style_node"),
                base_image_name  # output_subfolder basado en imagen original, no en workflow
            )
            
            # Enviar a ComfyUI (sin esperar respuesta)
            log_info(f"üöÄ Enviando prompt {i+1}/{len(workflows)} a ComfyUI...")
            prompt_id = submit_workflow_to_comfyui(workflow)
            
            if prompt_id:
                submitted_prompts[prompt_id] = {
                    "workflow_info": workflow_info,
                    "unique_filename": unique_filename,
                    "submit_time": time.time(),
                    "index": i
                }
                log_success(f"‚úÖ Enviado {i+1}/{len(workflows)}: {workflow_info['id']} (prompt_id: {prompt_id})")
            else:
                log_error(f"‚ùå Error enviando {i+1}/{len(workflows)}: {workflow_info['id']}")
                result = {
                    "workflow_id": workflow_info["id"],
                    "workflow_info": workflow_info,
                    "success": False,
                    "error": "Error enviando workflow a ComfyUI",
                    "processing_time": 0
                }
                results.append(result)
                
                # Actualizar tracking inmediatamente
                with BATCH_LOCK:
                    if batch_id in ACTIVE_BATCHES:
                        batch_info = ACTIVE_BATCHES[batch_id]
                        batch_info["completed_workflows"] += 1
                        batch_info["failed"] += 1
                        batch_info["results"].append(result)
                
        except Exception as e:
            log_error(f"‚ùå Error preparando workflow {workflow_info['id']}: {str(e)}")
            result = {
                "workflow_id": workflow_info["id"],
                "workflow_info": workflow_info,
                "success": False,
                "error": f"Error preparando workflow: {str(e)}",
                "processing_time": 0
            }
            results.append(result)
            
            # Actualizar tracking inmediatamente
            with BATCH_LOCK:
                if batch_id in ACTIVE_BATCHES:
                    batch_info = ACTIVE_BATCHES[batch_id]
                    batch_info["completed_workflows"] += 1
                    batch_info["failed"] += 1
                    batch_info["results"].append(result)
    
    total_sending_time = time.time() - start_sending_time
    log_success(f"üì§ Todos los prompts enviados en {total_sending_time:.1f}s (promedio: {total_sending_time/len(workflows):.2f}s por prompt)")
    
    if not submitted_prompts:
        log_error("‚ùå No se pudo enviar ning√∫n workflow a ComfyUI")
        with BATCH_LOCK:
            if batch_id in ACTIVE_BATCHES:
                ACTIVE_BATCHES[batch_id]["status"] = "error"
                ACTIVE_BATCHES[batch_id]["error"] = "No se pudo enviar ning√∫n workflow"
        
        # Actualizar tambi√©n el job de sesi√≥n
        if session_job_id:
            session_manager.update_job(session_job_id, 
                status='error',
                error="No se pudo enviar ning√∫n workflow"
            )
        return results
    
    # Actualizar status: procesando
    with BATCH_LOCK:
        if batch_id in ACTIVE_BATCHES:
            ACTIVE_BATCHES[batch_id]["status"] = "processing"
            ACTIVE_BATCHES[batch_id]["current_operation"] = f"Procesando {len(submitted_prompts)} workflows..."
    
    # Actualizar tambi√©n el job de sesi√≥n
    if session_job_id:
        session_manager.update_job(session_job_id, 
            current_operation=f"Procesando {len(submitted_prompts)} workflows..."
        )
    
    log_info(f"üèÅ {len(submitted_prompts)} workflows enviados a ComfyUI. Esperando resultados...")
    
    # Fase 2: Esperar y recoger resultados simult√°neamente CON TRACKING
    def wait_for_single_workflow_with_tracking(prompt_id, workflow_data, master_image):
        """Espera el resultado de un workflow espec√≠fico CON TRACKING"""
        try:
            start_time = workflow_data["submit_time"]
            workflow_info = workflow_data["workflow_info"]
            index = workflow_data["index"]
            
            log_info(f"‚è≥ Esperando resultado {index+1}: {workflow_info['id']} (prompt_id: {prompt_id})")
            
            # Actualizar status individual
            with BATCH_LOCK:
                if batch_id in ACTIVE_BATCHES:
                    ACTIVE_BATCHES[batch_id]["current_operation"] = f"Procesando: {workflow_info['id']}"
            
            # Esperar completion de este workflow espec√≠fico
            outputs = wait_for_completion(prompt_id, timeout=60000) # 10 minutos timeout
            
            # Extraer im√°genes generadas
            original_image_name = common_params.get('original_filename', 'batch_image')
            include_upscale = common_params.get('include_upscale', True)
            generated_images = extract_generated_images(outputs, original_image_name, include_upscale)
            
            # Obtener nombre base de la imagen original usando la misma l√≥gica que process_image individual
            original_image_name = common_params.get('original_filename', 'batch_image')
            base_image_name = secure_filename(original_image_name.rsplit('.', 1)[0] if '.' in original_image_name else 'image')
            
            log_info(f"üìÅ Procesando batch - original: '{original_image_name}' -> base: '{base_image_name}'")
            
            # Crear directorio de salida basado en nombre de imagen original (shared)
            batch_output_dir = create_output_directory(base_image_name)
            
            # üîç VERIFICAR ARCHIVOS EXISTENTES EN EL DIRECTORIO PARA EVITAR DUPLICADOS DE UPSCALE
            existing_files = []
            if os.path.exists(batch_output_dir):
                existing_files = [f.lower() for f in os.listdir(batch_output_dir)]
            
            # Verificar si ya existe una imagen upscale
            existing_upscale = any('upscale_' in f for f in existing_files)
            if existing_upscale:
                log_info(f"üìà Ya existe una imagen upscale en {batch_output_dir}, se omitir√°n nuevas im√°genes upscale")
            
            # Copiar im√°genes generadas con nombre de workflow y estilo (evitando duplicados)
            # Las im√°genes en generated_images ya est√°n filtradas seg√∫n include_upscale
            saved_images = []
            session_images = []  # Para URLs de sesi√≥n
            include_upscale = common_params.get('include_upscale', True)
            log_info(f"üìä Tracking Batch - Resumen de im√°genes extra√≠das: {len(generated_images)} im√°genes (include_upscale={include_upscale})")
            for i, img in enumerate(generated_images):
                log_info(f"   {i+1}. Tipo: {img.get('image_type', 'unknown')}, Archivo: {img['filename']}, Nodo: {img.get('node_id', 'N/A')}")
            
            for img_info in generated_images:
                
                source_path = find_image_file(img_info['filename'], img_info['subfolder'])
                if source_path:
                    # Obtener extensi√≥n original
                    if '.' in img_info['filename']:
                        original_ext = img_info['filename'].rsplit('.', 1)[1]
                    else:
                        original_ext = 'png'
                    
                    # Diferentes estrategias de naming seg√∫n el tipo de imagen
                    image_type = img_info.get('image_type', 'unknown')
                    
                    # üîç LOG DETALLADO PARA DEBUGGING EN BATCH TRACKING
                    log_info(f"üîç BATCH TRACKING - Procesando imagen: {img_info['filename']}")
                    log_info(f"   üìã Tipo detectado: {image_type}")
                    log_info(f"   üìã Nodo origen: {img_info.get('node_id', 'N/A')}")
                    log_info(f"   üìã Workflow: {workflow_info['id']}")
                    log_info(f"   üìã Archivo original de ComfyUI: {img_info['filename']}")
                    log_info(f"   üìã Subfolder: {img_info.get('subfolder', 'N/A')}")
                    
                    if image_type == 'upscale':
                        # üîç VERIFICAR SI YA EXISTE UPSCALE ANTES DE PROCESAR
                        if existing_upscale:
                            log_warning(f"‚ö†Ô∏è Ya existe imagen upscale, saltando: {img_info['filename']}")
                            # Buscar el archivo upscale existente para referenciarlo
                            existing_upscale_file = next((f for f in os.listdir(batch_output_dir) if 'upscale_' in f.lower()), None)
                            if existing_upscale_file:
                                # Agregar referencia al archivo existente
                                saved_images.append({
                                    'filename': existing_upscale_file,
                                    'url': f"/get-image/{base_image_name}/{existing_upscale_file}",
                                    'original_filename': img_info['filename'],
                                    'workflow': workflow_info['id'],
                                    'image_type': image_type,
                                    'status': 'existing_previous'  # Marcar como existente de workflow anterior
                                })
                                log_info(f"üìà Referenciando upscale existente: {existing_upscale_file}")
                            continue
                        
                        # ‚úÖ UPSCALE: Nombre consistente basado en imagen original (no en workflow)
                        # Formato: upscale_nombreoriginal_timestamp.ext
                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                        new_filename = f"upscale_{base_image_name}_{timestamp}.{original_ext}"
                        
                        log_info(f"üìà BATCH TRACKING UPSCALE - Nombre generado: {new_filename}")
                        log_info(f"   üìà Raz√≥n: image_type='{image_type}' -> usando formato upscale_[base]_[timestamp]")
                    else:
                        # ‚úÖ COMPOSICI√ìN: Nombre √∫nico con workflow para distinguir diferentes composiciones
                        # Formato: workflow_estilo_timestamp_numero.ext
                        style_name = common_params.get('style', 'default')
                        workflow_clean = workflow_info['id'].replace('/', '_').replace('-', '_')
                        img_number = len(saved_images) + 1
                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                        new_filename = f"{workflow_clean}_{style_name}_{timestamp}_{img_number:03d}.{original_ext}"
                        
                        log_info(f"üéØ BATCH TRACKING COMPOSICI√ìN - Nombre generado: {new_filename}")
                        log_info(f"   üéØ Raz√≥n: image_type='{image_type}' -> usando formato [workflow]_[style]_[timestamp]_[num]")
                        log_info(f"   üéØ Componentes: workflow='{workflow_clean}', style='{style_name}', num={img_number:03d}")
                    
                    dest_path = os.path.join(batch_output_dir, new_filename)
                    
                    # ‚úÖ VERIFICAR SI YA EXISTE PARA EVITAR DUPLICADOS
                    if os.path.exists(dest_path):
                        log_warning(f"‚ö†Ô∏è Archivo de tracking ya existe, saltando: {new_filename}")
                        # Intentar guardarlo en sesi√≥n si no est√° ya guardado
                        session_url = None
                        if session_job_id:
                            try:
                                with open(dest_path, 'rb') as img_file:
                                    session_url = session_manager.save_job_image(session_job_id, img_file.read(), new_filename)
                            except Exception as e:
                                log_warning(f"‚ö†Ô∏è No se pudo guardar archivo existente en sesi√≥n: {str(e)}")
                        
                        saved_images.append({
                            'filename': new_filename,
                            'url': f"/get-image/{base_image_name}/{new_filename}",
                            'session_url': session_url,
                            'original_filename': img_info['filename'],
                            'workflow': workflow_info['id'],
                            'image_type': image_type,  # Agregar tipo de imagen
                            'status': 'existing'  # Marcar como existente
                        })
                        
                        if session_url:
                            session_images.append(session_url)
                        continue
                    
                    # Convertir PNG a JPG con l√≠mite de 200KB (igual que en save_images_to_our_output)
                    if original_ext.lower() == 'png':
                        # Cambiar extensi√≥n a JPG
                        new_filename = new_filename.replace('.png', '.jpg')
                        dest_path = os.path.join(batch_output_dir, new_filename)
                        
                        # Convertir PNG a JPG con optimizaci√≥n inteligente de tama√±o
                        try:
                            from PIL import Image
                            import io
                            
                            # Abrir imagen PNG original
                            img = Image.open(source_path)
                            original_size_kb = os.path.getsize(source_path) / 1024
                            
                            # Convertir a RGB si es necesario (PNG puede tener transparencia)
                            if img.mode in ('RGBA', 'LA', 'P'):
                                # Crear fondo blanco para im√°genes con transparencia
                                background = Image.new('RGB', img.size, (255, 255, 255))
                                if img.mode == 'P':
                                    img = img.convert('RGBA')
                                background.paste(img, mask=img.split()[-1] if img.mode == 'RGBA' else None)
                                img = background
                            elif img.mode != 'RGB':
                                img = img.convert('RGB')
                            
                            # Estrategia de optimizaci√≥n: Comenzar con calidad 100% y reducir gradualmente
                            target_size_kb = 200
                            
                            # Probar calidad 100% primero
                            buffer = io.BytesIO()
                            img.save(buffer, format='JPEG', quality=100, optimize=True)
                            size_kb = buffer.tell() / 1024
                            
                            if size_kb <= target_size_kb:
                                # ¬°Perfecto! Calidad 100% y dentro del l√≠mite
                                with open(dest_path, 'wb') as f:
                                    f.write(buffer.getvalue())
                                log_success(f"üéØ Imagen batch convertida a JPG con calidad 100%: {new_filename} ({size_kb:.1f}KB)")
                                log_info(f"   üìä Reducci√≥n de tama√±o: {original_size_kb:.1f}KB PNG ‚Üí {size_kb:.1f}KB JPG ({((original_size_kb - size_kb) / original_size_kb * 100):.1f}% reducci√≥n)")
                            else:
                                # Necesitamos reducir el tama√±o, pero manteniendo la mejor calidad posible
                                log_info(f"üìè Imagen batch muy grande con calidad 100% ({size_kb:.1f}KB), optimizando para 200KB...")
                                
                                best_quality = 100
                                best_buffer = None
                                
                                # Algoritmo de optimizaci√≥n m√°s inteligente: reducir en pasos m√°s peque√±os
                                for quality in range(95, 60, -2):  # Reducir de 2 en 2 desde 95% hasta 60%
                                    buffer = io.BytesIO()
                                    img.save(buffer, format='JPEG', quality=quality, optimize=True)
                                    size_kb = buffer.tell() / 1024
                                    
                                    if size_kb <= target_size_kb:
                                        best_quality = quality
                                        best_buffer = buffer.getvalue()
                                        break
                                
                                # Si a√∫n no encontramos una buena calidad, probar con pasos m√°s grandes
                                if best_buffer is None:
                                    log_warning(f"‚ö†Ô∏è Imagen batch muy grande, probando calidades m√°s bajas...")
                                    for quality in range(60, 30, -5):  # Reducir de 5 en 5 desde 60% hasta 30%
                                        buffer = io.BytesIO()
                                        img.save(buffer, format='JPEG', quality=quality, optimize=True)
                                        size_kb = buffer.tell() / 1024
                                        
                                        if size_kb <= target_size_kb:
                                            best_quality = quality
                                            best_buffer = buffer.getvalue()
                                            break
                                
                                # Guardar la mejor versi√≥n encontrada
                                if best_buffer:
                                    with open(dest_path, 'wb') as f:
                                        f.write(best_buffer)
                                    final_size = len(best_buffer) / 1024
                                    log_info(f"üéØ Imagen batch convertida a JPG: {new_filename} ({final_size:.1f}KB, calidad {best_quality}%)")
                                    log_info(f"   üìä Reducci√≥n de tama√±o: {original_size_kb:.1f}KB PNG ‚Üí {final_size:.1f}KB JPG ({((original_size_kb - final_size) / original_size_kb * 100):.1f}% reducci√≥n)")
                                else:
                                    # Como √∫ltimo recurso, guardar con calidad 30%
                                    img.save(dest_path, format='JPEG', quality=30, optimize=True)
                                    final_size = os.path.getsize(dest_path) / 1024
                                    log_warning(f"‚ö†Ô∏è Imagen batch muy grande, guardada con calidad 30%: {new_filename} ({final_size:.1f}KB)")
                                    log_info(f"   üìä Reducci√≥n de tama√±o: {original_size_kb:.1f}KB PNG ‚Üí {final_size:.1f}KB JPG ({((original_size_kb - final_size) / original_size_kb * 100):.1f}% reducci√≥n)")
                            
                        except Exception as e:
                            log_error(f"‚ùå Error convirtiendo PNG a JPG en batch: {str(e)}")
                            # Fallback: copiar archivo original
                            shutil.copy2(source_path, dest_path)
                    else:
                        # Copiar archivo no-PNG normalmente
                        shutil.copy2(source_path, dest_path)
                    
                    # Guardar tambi√©n en sesi√≥n
                    with open(dest_path, 'rb') as img_file:
                        session_url = session_manager.save_job_image(session_job_id, img_file.read(), new_filename)
                    
                    # Agregar a lista de guardadas
                    saved_images.append({
                        'filename': new_filename,
                        'url': f"/get-image/{base_image_name}/{new_filename}",
                        'session_url': session_url,  # URL de sesi√≥n persistente
                        'original_filename': img_info['filename'],
                        'workflow': workflow_info['id'],
                        'image_type': image_type,  # Agregar tipo de imagen
                        'status': 'new'  # Marcar como nuevo
                    })
                    
                    # Log espec√≠fico para upscales y composiciones en batch tracking
                    if image_type == 'upscale':
                        log_info(f"üìà BATCH TRACKING UPSCALE guardado con nombre consistente: {new_filename}")
                        # Marcar que ya existe upscale para evitar duplicados en siguientes workflows del batch
                        existing_upscale = True
                    else:
                        log_info(f"üéØ BATCH TRACKING COMPOSICI√ìN guardada con nombre √∫nico: {new_filename}")
                    
                    if session_url:
                        session_images.append(session_url)
            
            # Guardar imagen original solo una vez (check si ya existe) - convertir a JPG
            original_dest = os.path.join(batch_output_dir, 'original.jpg')  # Cambiar a JPG
            if not os.path.exists(original_dest):  # Solo si no existe ya
                try:
                    # Convertir imagen original a JPG con optimizaci√≥n
                    original_rgb = master_image.convert('RGB') if master_image.mode != 'RGB' else master_image
                    
                    # Optimizar imagen original a JPG con l√≠mite de 200KB
                    target_size_kb = 200
                    
                    # Intentar calidad 100% primero
                    buffer = io.BytesIO()
                    original_rgb.save(buffer, format='JPEG', quality=100, optimize=True)
                    size_kb = buffer.tell() / 1024
                    
                    if size_kb <= target_size_kb:
                        # Perfecto con calidad 100%
                        with open(original_dest, 'wb') as f:
                            f.write(buffer.getvalue())
                        log_success(f"‚úÖ Imagen original batch convertida a JPG con calidad 100%: original.jpg ({size_kb:.1f}KB)")
                    else:
                        # Optimizar gradualmente para mantener la mejor calidad posible
                        log_info(f"üìè Imagen original batch muy grande con calidad 100% ({size_kb:.1f}KB), optimizando para 200KB...")
                        
                        best_quality = 100
                        best_buffer = None
                        
                        # Optimizaci√≥n gradual m√°s inteligente
                        for quality in range(95, 60, -2):  # Reducir de 2 en 2 desde 95% hasta 60%
                            buffer = io.BytesIO()
                            original_rgb.save(buffer, format='JPEG', quality=quality, optimize=True)
                            size_kb = buffer.tell() / 1024
                            
                            if size_kb <= target_size_kb:
                                best_quality = quality
                                best_buffer = buffer.getvalue()
                                break
                        
                        # Si a√∫n no encontramos una buena calidad, probar con pasos m√°s grandes
                        if best_buffer is None:
                            for quality in range(60, 30, -5):  # Reducir de 5 en 5 desde 60% hasta 30%
                                buffer = io.BytesIO()
                                original_rgb.save(buffer, format='JPEG', quality=quality, optimize=True)
                                size_kb = buffer.tell() / 1024
                                
                                if size_kb <= target_size_kb:
                                    best_quality = quality
                                    best_buffer = buffer.getvalue()
                                    break
                        
                        # Guardar la mejor versi√≥n encontrada
                        if best_buffer:
                            with open(original_dest, 'wb') as f:
                                f.write(best_buffer)
                            final_size = len(best_buffer) / 1024
                            log_info(f"‚úÖ Imagen original batch convertida a JPG: original.jpg ({final_size:.1f}KB, calidad {best_quality}%)")
                        else:
                            # Como √∫ltimo recurso, guardar con calidad 30%
                            original_rgb.save(original_dest, format='JPEG', quality=30, optimize=True)
                            final_size = os.path.getsize(original_dest) / 1024
                            log_warning(f"‚ö†Ô∏è Imagen original batch muy grande, guardada con calidad 30%: original.jpg ({final_size:.1f}KB)")
                    
                    log_info(f"üíæ Imagen original batch guardada como JPG: {original_dest}")
                    
                except Exception as e:
                    log_error(f"‚ùå Error convirtiendo imagen original batch a JPG: {str(e)}")
                    # Fallback: guardar como PNG
                    original_dest = os.path.join(batch_output_dir, 'original.png')
                    master_image.save(original_dest, format='PNG')
                    log_info(f"üíæ Imagen original batch guardada como PNG (fallback): {original_dest}")
            
            # Guardar tambi√©n imagen original en la sesi√≥n (solo una vez por batch)
            original_session_url = None
            if session_job_id:
                try:
                    # Verificar si ya se guard√≥ la original en sesi√≥n
                    existing_images = session_manager.get_job_images(session_job_id)
                    original_already_saved = any('original.' in img for img in existing_images)  # Buscar tanto PNG como JPG
                    
                    if not original_already_saved:
                        with open(original_dest, 'rb') as img_file:
                            original_filename = os.path.basename(original_dest)  # original.jpg o original.png
                            original_session_url = session_manager.save_job_image(session_job_id, img_file.read(), original_filename)
                except Exception as e:
                    log_warning(f"‚ö†Ô∏è No se pudo guardar imagen original en sesi√≥n: {str(e)}")
            
            processing_time = time.time() - start_time
            
            # üéØ FILTRAR IM√ÅGENES PARA EL FRONTEND EN BATCH TRACKING - SOLO COMPOSICI√ìN FINAL
            log_info(f"üéØ Filtrando im√°genes para frontend en batch tracking - solo composici√≥n final...")
            frontend_images = []
            
            # En batch tracking, filtrar para mostrar solo la primera imagen (composici√≥n)
            if saved_images:
                frontend_images = [saved_images[0]]  # Solo la primera (composici√≥n)
                log_info(f"‚úÖ Batch tracking - Imagen de composici√≥n para frontend: {saved_images[0]['filename']}")
            
            log_info(f"üì§ Batch tracking - Para frontend: {len(frontend_images)} imagen(es) de {len(saved_images)} guardadas")
            
            result = {
                "workflow_id": workflow_info["id"],
                "workflow_info": workflow_info,
                "success": True,
                "generated_images": frontend_images,  # ‚úÖ SOLO IM√ÅGENES PARA FRONTEND (composici√≥n √∫nicamente)
                "all_images_saved": saved_images,  # Todas las im√°genes guardadas en disco (para debugging)
                "session_images": session_images,  # URLs de sesi√≥n persistentes
                "original_image": {
                    "filename": "original.png",
                    "url": f"/get-image/{base_image_name}/original.png",
                    "session_url": original_session_url  # URL de sesi√≥n para imagen original
                },
                "processing_time": round(processing_time, 2),
                "prompt_id": prompt_id,
                "completion_time": datetime.now().isoformat()
            }
            
            log_success(f"‚úÖ Completado {index+1}: {workflow_info['id']} en {processing_time:.1f}s ({len(saved_images)} im√°genes)")
            
            # *** ACTUALIZAR TRACKING INMEDIATAMENTE CUANDO TERMINA ***
            with BATCH_LOCK:
                if batch_id in ACTIVE_BATCHES:
                    batch_info = ACTIVE_BATCHES[batch_id]
                    batch_info["completed_workflows"] += 1
                    batch_info["successful"] += 1
                    batch_info["results"].append(result)
                    
                    # Calcular estimaci√≥n de tiempo restante
                    if batch_info["completed_workflows"] > 0:
                        elapsed_time = time.time() - batch_info["start_time"]
                        avg_time_per_workflow = elapsed_time / batch_info["completed_workflows"]
                        remaining_workflows = batch_info["total_workflows"] - batch_info["completed_workflows"]
                        estimated_remaining = avg_time_per_workflow * remaining_workflows
                        batch_info["estimated_completion"] = time.time() + estimated_remaining
                    
                    batch_info["current_operation"] = f"Completado: {workflow_info['id']} ({batch_info['completed_workflows']}/{batch_info['total_workflows']})"
            
            # *** ACTUALIZAR TAMBI√âN EL JOB DE SESI√ìN ***
            if session_job_id:
                # Obtener estado actual del batch
                with BATCH_LOCK:
                    if batch_id in ACTIVE_BATCHES:
                        batch_info = ACTIVE_BATCHES[batch_id]
                        session_manager.update_job(session_job_id,
                            completed_workflows=batch_info["completed_workflows"],
                            successful=batch_info["successful"],
                            failed=batch_info["failed"],
                            current_operation=batch_info["current_operation"],
                            results=batch_info["results"][:10]  # Solo los √∫ltimos 10 para no sobrecargar
                        )
            
            return result
            
        except Exception as e:
            processing_time = time.time() - workflow_data["submit_time"]
            log_error(f"‚ùå Error procesando {workflow_info['id']}: {str(e)}")
            
            result = {
                "workflow_id": workflow_info["id"],
                "workflow_info": workflow_info,
                "success": False,
                "error": str(e),
                "processing_time": round(processing_time, 2),
                "prompt_id": prompt_id,
                "completion_time": datetime.now().isoformat()
            }
            
            # *** ACTUALIZAR TRACKING INMEDIATAMENTE EN CASO DE ERROR ***
            with BATCH_LOCK:
                if batch_id in ACTIVE_BATCHES:
                    batch_info = ACTIVE_BATCHES[batch_id]
                    batch_info["completed_workflows"] += 1
                    batch_info["failed"] += 1
                    batch_info["results"].append(result)
                    batch_info["current_operation"] = f"Error en: {workflow_info['id']} ({batch_info['completed_workflows']}/{batch_info['total_workflows']})"
            
            # *** ACTUALIZAR TAMBI√âN EL JOB DE SESI√ìN EN CASO DE ERROR ***
            if session_job_id:
                # Obtener estado actual del batch
                with BATCH_LOCK:
                    if batch_id in ACTIVE_BATCHES:
                        batch_info = ACTIVE_BATCHES[batch_id]
                        session_manager.update_job(session_job_id,
                            completed_workflows=batch_info["completed_workflows"],
                            successful=batch_info["successful"],
                            failed=batch_info["failed"],
                            current_operation=batch_info["current_operation"],
                            results=batch_info["results"][:10]  # Solo los √∫ltimos 10 para no sobrecargar
                        )
            
            return result
    
    # Usar ThreadPoolExecutor para esperar todos los workflows en paralelo
    max_workers = min(len(submitted_prompts), 10)  # M√°ximo 10 hilos concurrentes
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Enviar todas las tareas de espera (crear copia de imagen para cada hilo)
        future_to_prompt = {
            executor.submit(wait_for_single_workflow_with_tracking, prompt_id, workflow_data, master_image.copy()): prompt_id 
            for prompt_id, workflow_data in submitted_prompts.items()
        }
        
        # Recoger resultados conforme van complet√°ndose
        for future in concurrent.futures.as_completed(future_to_prompt):
            prompt_id = future_to_prompt[future]
            try:
                result = future.result()
                results.append(result)
                completed = len(results)
                total = len(submitted_prompts)
                log_info(f"üìà Progreso: {completed}/{total} workflows completados")
            except Exception as e:
                workflow_data = submitted_prompts[prompt_id]
                workflow_info = workflow_data["workflow_info"]
                log_error(f"‚ùå Error obteniendo resultado de {workflow_info['id']}: {str(e)}")
                error_result = {
                    "workflow_id": workflow_info["id"],
                    "workflow_info": workflow_info,
                    "success": False,
                    "error": f"Error obteniendo resultado: {str(e)}",
                    "processing_time": 0,
                    "prompt_id": prompt_id
                }
                results.append(error_result)
                
                # Actualizar tracking
                with BATCH_LOCK:
                    if batch_id in ACTIVE_BATCHES:
                        batch_info = ACTIVE_BATCHES[batch_id]
                        batch_info["completed_workflows"] += 1
                        batch_info["failed"] += 1
                        batch_info["results"].append(error_result)
    
    # Ordenar resultados por el √≠ndice original para mantener orden
    results.sort(key=lambda x: next(
        (data["index"] for prompt_id, data in submitted_prompts.items() 
         if x.get("prompt_id") == prompt_id), 999
    ))
    
    return results

# ==================== ENDPOINTS DE CANCELACI√ìN ====================

@app.route('/comfyui/queue/status', methods=['GET'])
def get_queue_status():
    """Obtiene el estado actual de la cola de ComfyUI"""
    try:
        log_info("üìä Consultando estado de cola de ComfyUI...")
        queue_status = get_comfyui_queue_status()
        
        return jsonify({
            "success": True,
            "queue_status": queue_status,
            "message": f"Cola: {queue_status.get('running_count', 0)} ejecut√°ndose, {queue_status.get('queued_count', 0)} pendientes"
        })
        
    except Exception as e:
        log_error(f"Error consultando estado de cola: {str(e)}")
        return jsonify({
            "success": False,
            "error": str(e)
        }), 500

@app.route('/comfyui/cancel/all', methods=['POST'])
def cancel_all_jobs():
    """Cancela todos los trabajos en ComfyUI (en ejecuci√≥n y en cola)"""
    try:
        log_info("üö® Solicitud de cancelaci√≥n masiva recibida")
        
        result = cancel_all_comfyui_jobs()
        
        if result['success']:
            log_success(f"‚úÖ Cancelaci√≥n exitosa: {result['message']}")
            return jsonify(result)
        else:
            log_error(f"‚ùå Error en cancelaci√≥n: {result.get('error', 'Error desconocido')}")
            return jsonify(result), 500
            
    except Exception as e:
        log_error(f"Error durante cancelaci√≥n masiva: {str(e)}")
        return jsonify({
            "success": False,
            "error": str(e)
        }), 500

@app.route('/comfyui/cancel/<prompt_id>', methods=['POST'])
def cancel_specific_job(prompt_id):
    """Cancela un trabajo espec√≠fico por su ID"""
    try:
        log_info(f"üö® Solicitud de cancelaci√≥n para trabajo: {prompt_id}")
        
        success = cancel_specific_comfyui_job(prompt_id)
        
        if success:
            log_success(f"‚úÖ Trabajo {prompt_id} cancelado exitosamente")
            return jsonify({
                "success": True,
                "message": f"Trabajo {prompt_id} cancelado",
                "prompt_id": prompt_id
            })
        else:
            log_error(f"‚ùå No se pudo cancelar trabajo {prompt_id}")
            return jsonify({
                "success": False,
                "error": f"No se pudo cancelar el trabajo {prompt_id}",
                "prompt_id": prompt_id
            }), 500
            
    except Exception as e:
        log_error(f"Error cancelando trabajo {prompt_id}: {str(e)}")
        return jsonify({
            "success": False,
            "error": str(e),
            "prompt_id": prompt_id
        }), 500

# ==================== INICIO DEL SERVIDOR ====================

if __name__ == '__main__':
    import os
    # Solo ejecutar el arranque si es el proceso principal del reloader
    if os.environ.get('WERKZEUG_RUN_MAIN') == 'true' or not app.debug:
        log_info("üöÄ Iniciando ComfyUI API REST v2.0.0...")
        log_info(f"üìÅ Directorio de workflows: {WORKFLOWS_DIR}")
        log_info(f"üìÅ Directorio de input: {COMFYUI_INPUT_DIR}")
        log_info(f"üìÅ Directorio de output: {COMFYUI_OUTPUT_DIR}")
        log_info(f"üåê ComfyUI URL: {COMFYUI_URL}")
        # Verificar conexi√≥n con ComfyUI
        try:
            response = requests.get(f"{COMFYUI_URL}/system_stats", timeout=5)
            if response.status_code == 200:
                log_success("Conexi√≥n exitosa con ComfyUI")
            else:
                log_warning(f"No se pudo conectar con ComfyUI: status {response.status_code}")
        except Exception as e:
            log_warning(f"‚ö†Ô∏è No se pudo conectar con ComfyUI: {str(e)}")
        # Contar workflows disponibles
        try:
            workflows = []
            if os.path.exists(WORKFLOWS_DIR):
                for root, dirs, files in os.walk(WORKFLOWS_DIR):
                    for filename in files:
                        if filename.endswith('.json'):
                            workflows.append(filename)
            log_info(f"üìä {len(workflows)} workflows encontrados")
        except Exception as e:
            log_warning(f"‚ö†Ô∏è Error contando workflows: {str(e)}")
        # Iniciar servidor
        log_info("üåü Servidor iniciado en http://localhost:5000")
        log_info("üì± Cliente web disponible en: http://localhost:5000")
        app.run(host='0.0.0.0', port=5000, debug=True)
